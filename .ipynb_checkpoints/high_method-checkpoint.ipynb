{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from string import punctuation\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "import re\n",
    "import csv\n",
    "from gensim.summarization import bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_QUESTION_WORDS = ['what','who','whose','whom','where','when','why','how',\n",
    "                       'which',\"what's\",\"who's\",\"where's\",\"how's\"]\n",
    "CLOSED_QUESTION_WORDS = ['is','are','am','was','were','do','does,','did','can',\n",
    "                         'could','will','would','shall','should','have','has',\n",
    "                         'had']\n",
    "\n",
    "# Stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "lmtz = WordNetLemmatizer()\n",
    "\n",
    "with open('testing.json') as json_data:\n",
    "    test = json.load(json_data)\n",
    "\n",
    "with open('documents.json') as json_data:\n",
    "    documents = json.load(json_data)\n",
    "\n",
    "# Spacy toolkit\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "punc = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(token):\n",
    "    lemma = lmtz.lemmatize(token, 'v')\n",
    "    if lemma == token:\n",
    "        lemma = lmtz.lemmatize(token, 'n')\n",
    "    return lemma\n",
    "\n",
    "        \n",
    "def extract_term_freqs(doc):\n",
    "    tfs = {}\n",
    "    for token in nltk.word_tokenize(doc):\n",
    "        lemma = lemmatize(token.lower())\n",
    "        if lemma not in stop and lemma.isalpha():\n",
    "            tfs[lemma] = tfs.get(lemma, 0) + 1\n",
    "    return tfs\n",
    "\n",
    "\n",
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    dfs = Counter()\n",
    "    for tfs in doc_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            dfs[term] += 1\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def query_vsm(query, index, k=5):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "\n",
    "# Find the question word\n",
    "def get_qword(question):\n",
    "    tokens = nltk.word_tokenize(question.lower())\n",
    "    for token in tokens:\n",
    "        if token in OPEN_QUESTION_WORDS:\n",
    "            return token\n",
    "    for token in tokens:\n",
    "        if token in CLOSED_QUESTION_WORDS:\n",
    "            return token\n",
    "    return 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of longest same sequences of keywords\n",
    "def get_overlap(sent1, sent2):\n",
    "    tokens1 = []\n",
    "    tokens2 = []\n",
    "\n",
    "    for token in nltk.word_tokenize(strip_punctuation(sent1.lower())):\n",
    "        lemma = lemmatize(token)\n",
    "        if lemma not in stop:\n",
    "            tokens1.append(lemma)\n",
    "\n",
    "    for token in nltk.word_tokenize(strip_punctuation(sent2.lower())):\n",
    "        lemma = lemmatize(token)\n",
    "        if lemma not in stop:\n",
    "            tokens2.append(lemma)\n",
    "\n",
    "    max = 0\n",
    "    for i in range(len(tokens1)):\n",
    "        for j in range(len(tokens2)):\n",
    "\n",
    "            if tokens1[i] == tokens2[j]:\n",
    "                length = 1\n",
    "\n",
    "                ii = i + 1\n",
    "                jj = j + 1\n",
    "                while ii < len(tokens1) and jj < len(tokens2) and \\\n",
    "                        tokens1[ii] == tokens2[jj]:\n",
    "                    ii += 1\n",
    "                    jj += 1\n",
    "                    length += 1\n",
    "\n",
    "                if length > max:\n",
    "                    max = length\n",
    "\n",
    "    return max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   combination\n",
      "1   addition\n",
      "2   browser's layout engine\n",
      "3   internet explorer\n",
      "4   late 2004\n",
      "5   windows\n",
      "6   1990\n",
      "7   browsers\n",
      "8   marc andreessen\n",
      "9   first web browser\n",
      "10   competition\n",
      "11   dominance\n",
      "12   internet relay chat\n",
      "13   january\n",
      "14   every major web browser\n",
      "15   january 2003\n",
      "16   january 2009\n",
      "17   file transfer protocol\n",
      "18   google\n",
      "19   case\n",
      "20   rich user interfaces\n",
      "21   2002\n",
      "22   august 2011\n",
      "23   major web browsers\n",
      "24   chrome's user-base\n",
      "25   development\n",
      "26   december 2011\n",
      "27   1993\n",
      "28   netscape\n",
      "29   apple's safari\n",
      "30   rapid development\n",
      "31   prefix\n",
      "32   mozilla foundation\n",
      "33   private networks\n",
      "34   mac\n",
      "35   comparison\n",
      "36   1994\n",
      "37   user interface\n",
      "38   addition\n",
      "39   major browsers\n",
      "40   information resources\n",
      "41   live bookmarks\n",
      "42   more traditional feed reader\n",
      "43   bookmarks\n",
      "44   september 2008\n",
      "45   prefix\n",
      "46   browser software\n",
      "47   file transfer protocol\n",
      "48   microsoft corp\n",
      "49   mobile safari\n",
      "50   web browsers\n",
      "51   windows\n",
      "52   user's default e-mail application\n",
      "53   browser extension\n",
      "54   4.5 hour\n",
      "55   carrefour boycott\n",
      "56   chinese media\n",
      "57   protests\n",
      "58   several protests\n",
      "59   noted indian social activist\n",
      "60   san francisco's\n",
      "61   several protests\n",
      "62   site\n",
      "63   western media\n",
      "64   uk\n",
      "65   political activist\n",
      "66   russia\n",
      "67   distinction\n",
      "68   relay supporters\n",
      "69   arrival\n",
      "70   team\n",
      "71   public\n",
      "72   foreign minister stephen smith\n",
      "73   french members\n",
      "74   interior michèle alliot-marie\n",
      "75   australian police\n",
      "76   april 5\n",
      "77   members\n",
      "78   april 1, 2008\n",
      "79   april 16\n",
      "80   nagano\n",
      "81   accountability\n",
      "82   april 5\n",
      "83   total\n",
      "84   10 downing street\n",
      "85   torch\n",
      "86   china\n",
      "87   2012\n",
      "88   north korea\n",
      "89   750,000\n",
      "90   libération\n",
      "91   lhasa riots\n",
      "92   protests\n",
      "93   seoul\n",
      "94   libération\n",
      "95   giant banner\n",
      "96   route\n",
      "97   seven\n",
      "98   uyghurs\n",
      "99   reporters without borders'\n",
      "100   macau\n",
      "101   vietnamese american\n",
      "102   great britain\n",
      "103   event\n",
      "104   chinese media\n",
      "105   asian times\n",
      "106   india\n",
      "107   site\n",
      "108   libération\n",
      "109   indian\n",
      "110   costanera sur\n",
      "111   original 20 km relay\n",
      "112   1200\n",
      "113   assurances\n",
      "114   man\n",
      "115   foreign minister stephen smith\n",
      "116   major setback\n",
      "117   democracy\n",
      "118   tokyo\n",
      "119   kazakhstan\n",
      "120   paris\n",
      "121   reports\n",
      "122   praise\n",
      "123   air china\n",
      "124   hollywood\n",
      "125   march 2008\n",
      "126   jon stanhope\n",
      "127   oman\n",
      "128   ted quinlan\n",
      "129   several protests\n",
      "130   may 2\n",
      "131   kuala lumpur\n",
      "132   \n",
      "133   china\n",
      "134   tsim sha tsui\n",
      "135   new starting point\n",
      "136   event\n",
      "137   coverage\n",
      "138   chaotic torch relays\n",
      "139   many as 1,000\n",
      "140   event\n",
      "141   may 3\n",
      "142   thupten gyatso\n",
      "143   april 17\n",
      "144   protests\n",
      "145   one\n",
      "146   francesca martinez\n",
      "147   chinese\n",
      "148   demonstrators\n",
      "149   april 5\n",
      "150   2,200 police\n",
      "151   120\n",
      "152   response\n",
      "153   relay\n",
      "154   libération\n",
      "155   vietnamese american\n",
      "156   2008 summer olympics torch relay\n",
      "157   intended torchbearers choi seung-kook\n",
      "158   minister\n",
      "159   thousands\n",
      "160   1964\n",
      "161   justin herman plaza\n",
      "162   april 5\n",
      "163   francesca martinez\n",
      "164   number\n",
      "165   first time\n",
      "166   ali mohamed shein\n",
      "167   daily telegraph\n",
      "168   susan prager\n",
      "169   torch\n",
      "170   united nations organization\n",
      "171   jackie chan\n",
      "172   times\n",
      "173   francesca martinez\n",
      "174   kazakhstan\n",
      "175   great britain\n",
      "176   april\n",
      "177   1964\n",
      "178   invitees\n",
      "179   distinction\n",
      "180   torch\n",
      "181   two\n",
      "182   jane birkin\n",
      "183   one\n",
      "184   event\n",
      "185   may 2\n",
      "186   torch\n",
      "187   april\n",
      "188   france\n",
      "189   event\n",
      "190   torch relay route\n",
      "191   china\n",
      "192   70\n",
      "193   one\n",
      "194   85,000\n",
      "195   nagano\n",
      "196   india\n",
      "197   jane birkin\n",
      "198   torch\n",
      "199   day\n",
      "200   1964\n",
      "201   libération\n",
      "202   nathan road\n",
      "203   intended torchbearers choi seung-kook\n",
      "204   air china\n",
      "205   protests\n",
      "206   major protests\n",
      "207   paris city officials\n",
      "208   turkey\n",
      "209   japanese family\n",
      "210   taipei\n",
      "211   baichung bhutia\n",
      "212   malaysia\n",
      "213   call\n",
      "214   april 3\n",
      "215   politicians\n",
      "216   several protests\n",
      "217   chinese demands\n",
      "218   carrefour\n",
      "219   days\n",
      "220   china's relations\n",
      "221   oman\n",
      "222   \"immigration reasons\n",
      "223   two\n",
      "224   april 17\n",
      "225   members\n",
      "226   macau\n",
      "227   lê minh phiếu\n",
      "228   kazakhstan\n",
      "229   jacques rogge\n",
      "230   several protests\n",
      "231   several protests\n",
      "232   april\n",
      "233   sacred torch\n",
      "234   one\n",
      "235   15 minutes\n",
      "236   medeo\n",
      "237   seven\n",
      "238   call\n",
      "239   aluminum\n",
      "240   kazakhstan\n",
      "241   relay\n",
      "242   \n",
      "243   chinese media\n",
      "244   vietnam\n",
      "245   saint petersburg\n",
      "246   government\n",
      "247   may 3\n",
      "248   eiffel tower\n",
      "249   tanzania\n",
      "250   beijing\n",
      "251   air china\n",
      "252   tibetan flags\n",
      "253   two\n",
      "254   1200\n",
      "255   days\n",
      "256   great britain\n",
      "257   april\n",
      "258   two\n",
      "259   several protests\n",
      "260   paris\n",
      "261   president pervez musharraf\n",
      "262   macau\n",
      "263   150,000-strong\n",
      "264   carrefour\n",
      "265   protests\n",
      "266   bollywood actress soha ali khan\n",
      "267   chinese media coverage\n",
      "268   great britain\n",
      "269   middle east\n",
      "270   india\n",
      "271   macau\n",
      "272   ancient links\n",
      "273   tony goh\n",
      "274   government\n",
      "275   reporters\n",
      "276   people\n",
      "277   response\n",
      "278   istanbul\n",
      "279   torchbearers\n",
      "280   narisa chakrabongse, green world foundation (gwf) chairwoman\n",
      "281   macao daily news\n",
      "282   paralympic games\n",
      "283   may 4\n",
      "284   call\n",
      "285   parliament\n",
      "286   great britain\n",
      "287   bangkok\n",
      "288   several symbolic protests\n",
      "289   libération\n",
      "290   april 16\n",
      "291   chinese government\n",
      "292   buenos aires\n",
      "293   trocadéro\n",
      "294   torchbearers\n",
      "295   trocadéro\n",
      "296   people's daily\n",
      "297   april 9\n",
      "298   tibetan flags\n",
      "299   prior\n",
      "300   japan\n",
      "301   chinese officials\n",
      "302   several times\n",
      "303   torch relay route\n",
      "304   japan\n",
      "305   relay\n",
      "306   china\n",
      "307   several hundred pro-tibet protesters\n",
      "308   several protests\n",
      "309   intended torchbearer lin hatfield dodds\n",
      "310   85,000\n",
      "311   peter ueberroth\n",
      "312   three activists\n",
      "313   carrefour\n",
      "314   original 20 km relay\n",
      "315   pyongyang\n",
      "316   than 100\n",
      "317   south korea\n",
      "318   chan\n",
      "319   total\n",
      "320   baichung bhutia\n",
      "321   relay\n",
      "322   march 2008\n",
      "323   torch\n",
      "324   1988\n",
      "325   tokyo\n",
      "326   30\n",
      "327   arrival\n",
      "328   65 kilometre per hour\n",
      "329   san francisco\n",
      "330   latin america\n",
      "331   islamabad\n",
      "332   torch\n",
      "333   kazakhstan\n",
      "334   islamabad\n",
      "335   129 days\n",
      "336   fu ying\n",
      "337   april 1, 2008\n",
      "338   torch relay leg\n",
      "339   world ’ s worst offenders\n",
      "340   jakarta\n",
      "341   to 600\n",
      "342   to 600\n",
      "343   lê minh phiếu\n",
      "344   united states of america\n",
      "345   torch\n",
      "346   event\n",
      "347   protests\n",
      "348   kamal nath\n",
      "349   april 16\n",
      "350   tokyo\n",
      "351   torch\n",
      "352   bbc's paul danahar\n",
      "353   same pitch\n",
      "354   total number\n",
      "355   pitch\n",
      "356   12\n",
      "357   indefinite pitch\n",
      "358   100 cents\n",
      "359   frequency\n",
      "360   \"high\" pitch\n",
      "361   least one model\n",
      "362   overtones\n",
      "363   time\n",
      "364   a4\n",
      "365   the higher frequencies\n",
      "366   12\n",
      "367   perceived interval\n",
      "368   sound\n",
      "369   theories\n",
      "370   southampton\n",
      "371   uk\n",
      "372   annual southampton boat show\n",
      "373   seven\n",
      "374   city\n",
      "375   120,305\n",
      "376   sizeable polish population\n",
      "377   lloyd's register group\n",
      "378   most successful college american football teams\n",
      "379   1996\n",
      "380   great war\n",
      "381   9th century\n",
      "382   london\n",
      "383   new forest\n",
      "384   10th century\n",
      "385   southampton\n",
      "386   southampton city council\n",
      "387   roughly half\n",
      "388   2015 council elections\n",
      "389   than 2,000\n",
      "390   centre\n",
      "391   over a week\n",
      "392   portsmouth\n",
      "393   one\n",
      "394   walls\n",
      "395   city\n",
      "396   importance\n",
      "397   1888\n",
      "398   november 2014\n",
      "399   annual\n",
      "400   one third\n",
      "401   annual southampton boat show\n",
      "402   population\n",
      "403   evidence\n",
      "404   latter half of the 20th century\n",
      "405   october 2014\n",
      "406   settlement\n",
      "407   15th century\n",
      "408   1940s\n",
      "409   southampton's largest retail centre\n",
      "410   town\n",
      "411   city walls\n",
      "412   24\n",
      "413   2004\n",
      "414   two\n",
      "415   first half of the 20th century\n",
      "416   1964\n",
      "417   worst behaved secondary schools\n",
      "418   part\n",
      "419   four\n",
      "420   1840\n",
      "421   1862\n",
      "422   city council\n",
      "423   western docks\n",
      "424   space\n",
      "425   1417\n",
      "426   western docks\n",
      "427   1339\n",
      "428   202\n",
      "429   1976\n",
      "430   city\n",
      "431   presence\n",
      "432   centre\n",
      "433   1968\n",
      "434   port\n",
      "435   hampshire\n",
      "436   1066\n",
      "437   british gas\n",
      "438   west quay\n",
      "439   royston smith\n",
      "440   \n",
      "441   parts\n",
      "442   eastleigh\n",
      "443   princess alexandra dock\n",
      "444   vast majority\n",
      "445   classical concerts\n",
      "446   queen mary\n",
      "447   southampton\n",
      "448   early may\n",
      "449   uni-link\n",
      "450   cent\n",
      "451   third ferry\n",
      "452   university of southampton\n",
      "453   13th century\n",
      "454   provide\n",
      "455   1966\n",
      "456   hampshire\n",
      "457   west quay\n",
      "458   headquarters\n",
      "459   city\n",
      "460   additional police stations\n",
      "461   local train services\n",
      "462   two\n",
      "463   uk\n",
      "464   solent\n",
      "465   p&o cruises\n",
      "466   27 june 1640\n",
      "467   college\n",
      "468   9th century\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469   1339\n",
      "470   1233\n",
      "471   university hospital southampton nhs foundation trust\n",
      "472   biggest operator\n",
      "473   4.2\n",
      "474   1959\n",
      "475   354\n",
      "476   government figures\n",
      "477   m27\n",
      "478   coast\n",
      "479   hanover buildings\n",
      "480   city\n",
      "481   december 2007\n",
      "482   12th century\n",
      "483   traffic congestion\n",
      "484   southampton\n",
      "485   2004\n",
      "486   1233\n",
      "487   town\n",
      "488   three fire stations\n",
      "489   southampton docks\n",
      "490   24\n",
      "491   m27\n",
      "492   southampton's largest retail centre\n",
      "493   13th century\n",
      "494   1066\n",
      "495   clausentum\n",
      "496   two\n",
      "497   plans\n",
      "498   southampton\n",
      "499   port\n",
      "500   over a quarter\n",
      "501   main station\n",
      "502   20–24\n",
      "503   council estates\n",
      "504   south west trains\n",
      "505   16.2 percent\n",
      "506   king henry's departure\n",
      "507   route\n",
      "508   trust\n",
      "509   town\n",
      "510   16.2 percent\n",
      "511   two large live music venues\n",
      "512   newport\n",
      "513   university of southampton\n",
      "514   london\n",
      "515   river test and river itchen\n",
      "516   1968\n",
      "517   area\n",
      "518   hampshire county council\n",
      "519   duchess\n",
      "520   world's largest cruise ships\n",
      "521   1938\n",
      "522   large shopping centre\n",
      "523   uk\n",
      "524   2009\n",
      "525   southampton university\n",
      "526   1,000\n",
      "527   southampton operatic society\n",
      "528   1960s\n",
      "529   september 1940\n",
      "530   1854 book\n",
      "531   48\n",
      "532   plymouth\n",
      "533   town\n",
      "534   southampton water\n",
      "535   city\n",
      "536   12th century\n",
      "537   banister court stadium\n",
      "538   river itchen\n",
      "539   stone age\n",
      "540   1996 and 2004\n",
      "541   germany\n",
      "542   charters\n",
      "543   notable former player\n",
      "544   anglo-saxons\n",
      "545   hampshire constabulary\n",
      "546   seacity museum\n",
      "547   university hospital southampton nhs foundation trust\n",
      "548   itchen\n",
      "549   1966\n",
      "550   importance\n",
      "551   1835\n",
      "552   two\n",
      "553   1310\n",
      "554   southampton airport\n",
      "555   city\n",
      "556   great war\n",
      "557   18th-century\n",
      "558   23\n",
      "559   ocean race\n",
      "560   city-link\n",
      "561   two local sunday leagues\n",
      "562   part\n",
      "563   banister court stadium\n",
      "564   25–29\n",
      "565   crimean war\n",
      "566   three\n",
      "567   24\n",
      "568   city\n",
      "569   north\n",
      "570   unitary authority\n",
      "571   1310\n",
      "572   designer\n",
      "573   successive incarnations\n",
      "574   ikea\n",
      "575   docks\n",
      "576   france\n",
      "577   runners-up\n",
      "578   hampshire county\n",
      "579   \n",
      "580   variety\n",
      "581   hampshire constabulary\n",
      "582   southampton\n",
      "583   southampton solent university\n",
      "584   today\n",
      "585   january 2007\n",
      "586   southampton\n",
      "587   24 february 1964\n",
      "588   18th-century\n",
      "589   city\n",
      "590   march 2007\n",
      "591   university of southampton\n",
      "592   three\n",
      "593   john choules\n",
      "594   duchess\n",
      "595   48\n",
      "596   one\n",
      "597   coast\n",
      "598   controversy\n",
      "599   hampshire county council\n",
      "600   1760s\n",
      "601   hampshire county\n",
      "602   16.7\n",
      "603   uk\n",
      "604   500,000\n",
      "605   city\n",
      "606   plans\n",
      "607   today\n",
      "608   city\n",
      "609   11\n",
      "610   bargate\n",
      "611   southampton city college\n",
      "612   linda norris\n",
      "613   passenger services\n",
      "614   southampton\n",
      "615   port\n",
      "616   \n",
      "617   towns\n",
      "618   south hampshire\n",
      "619   october 1838\n",
      "620   one\n",
      "621   mud flats\n",
      "622   hampshire county\n",
      "623   1310\n",
      "624   hundreds\n",
      "625   walls\n",
      "626   london\n",
      "627   march 1644\n",
      "628   three\n",
      "629   woodmill bridge\n",
      "630   three\n",
      "631   river test and river itchen\n",
      "632   southampton pupils\n",
      "633   estimated 236,900 people\n",
      "634   site\n",
      "635   whitbread\n",
      "636   princess alexandra dock\n",
      "637   town\n",
      "638   council\n",
      "639   1944\n",
      "640   9th century\n",
      "641   june\n",
      "642   1310\n",
      "643   city\n",
      "644   southampton\n",
      "645   river test and river itchen\n",
      "646   last remains\n",
      "647   shirley\n",
      "648   eastleigh\n",
      "649   council\n",
      "650   importance\n",
      "651   uk\n",
      "652   england\n",
      "653   car manufacture\n",
      "654   october 2014\n",
      "655   1995 and 2004\n",
      "656   battle of agincourt\n",
      "657   2005\n",
      "658   nightclub\n",
      "659   uni-link\n",
      "660   may 1840\n",
      "661   october 2014\n",
      "662   southampton's largest retail centre\n",
      "663   january 2007\n",
      "664   common\n",
      "665   enterprise\n",
      "666   sunday\n",
      "667   bbc\n",
      "668   royston smith\n",
      "669   city\n",
      "670   barton peveril college\n",
      "671   southampton airport\n",
      "672   rest\n",
      "673   coast\n",
      "674   strong\n",
      "675   city hockey club\n",
      "676   river test and river itchen\n",
      "677   1996 and 2004\n",
      "678   two\n",
      "679   attempt\n",
      "680   remains\n",
      "681   major uk port\n",
      "682   supermarine spitfire\n",
      "683   local itv franchise\n",
      "684   city\n",
      "685   northern tip\n",
      "686   king henry's departure\n",
      "687   france\n",
      "688   \n",
      "689   uk\n",
      "690   swaythling\n",
      "691   premier league\n",
      "692   uk\n",
      "693   bitterne\n",
      "694   red funnel\n",
      "695   1938\n",
      "696   40,000\n",
      "697   crimean war\n",
      "698   previously\n",
      "699   1862\n",
      "700   1888\n",
      "701   steam yacht north star\n",
      "702   clausentum\n",
      "703   ferry port\n",
      "704   1879 to 1949\n",
      "705   itchen\n",
      "706   henry v's famous warship hms grace dieu\n",
      "707   1740\n",
      "708   southampton\n",
      "709   national lottery\n",
      "710   eastern docks\n",
      "711   royalists\n",
      "712   hampshire\n",
      "713   february 2011\n",
      "714   119,500 males\n",
      "715   nearly 92 %\n",
      "716   nearly 92 %\n",
      "717   eastern\n",
      "718   four women\n",
      "719   poland\n",
      "720   ashkenazi jew's\n",
      "721   40 %\n",
      "722   high frequencies\n",
      "723   however, most people\n",
      "724   1890s\n",
      "725   1990s\n",
      "726   middle east\n",
      "727   last thousand years\n",
      "728   probability\n",
      "729   ashkenazim\n",
      "730   nearly 92 %\n",
      "731   11th century\n",
      "732   last thousand years\n",
      "733   40 %\n",
      "734   often\n",
      "735   middle ages\n",
      "736   212\n",
      "737   certain issues\n",
      "738   midrash compilation\n",
      "739   jews\n",
      "740   several famous people\n",
      "741   holocaust\n",
      "742   20th century\n",
      "743   israeli black panthers\n",
      "744   study\n",
      "745   estimated 8.8 million\n",
      "746   israel\n",
      "747   hebrew\n",
      "748   possible exception\n",
      "749   roman empire\n",
      "750   recent years\n",
      "751   1750\n",
      "752   ashkenazi jews\n",
      "753   16th-century\n",
      "754   1095\n",
      "755   1095\n",
      "756   elevated frequency\n",
      "757   47.5 %\n",
      "758   concept\n",
      "759   uses\n",
      "760   2,500 years ago\n",
      "761   ancient levantine origins\n",
      "762   middle ages\n",
      "763   middle eastern peoples\n",
      "764   conformity\n",
      "765   archeological evidence\n",
      "766   several million\n",
      "767   ashkenazi jew\n",
      "768   hebrew\n",
      "769   pre-christian times\n",
      "770   period\n",
      "771   term\n",
      "772   center\n",
      "773   80 percent\n",
      "774   another trend\n",
      "775   centuries later\n",
      "776   11th century\n",
      "777   jews\n",
      "778   rabbinic leadership\n",
      "779   talmud\n",
      "780   modern-day italians\n",
      "781   30\n",
      "782   five thousand\n",
      "783   brigetio\n",
      "784   minhagim\n",
      "785   54 % –60 %\n",
      "786   genome-wide genetic study\n",
      "787   fertile crescent\n",
      "788   quarter\n",
      "789   1095\n",
      "790   second half of the 11th century\n",
      "791   jerusalem\n",
      "792   yiddish\n",
      "793   75 %\n",
      "794   yoma tractate\n",
      "795   conformity\n",
      "796   contemporary saadia gaon\n",
      "797   hebrew\n",
      "798   greek historian\n",
      "799   greek historian\n",
      "800   1095\n",
      "801   biblical figure\n",
      "802   first decades\n",
      "803   sergio dellapergola\n",
      "804   nearly 92 %\n",
      "805   jewish\n",
      "806   gomer\n",
      "807   80 percent\n",
      "808   jews\n",
      "809   ashkenazi jew\n",
      "810   2 %\n",
      "811   91 %\n",
      "812   upper euphrates\n",
      "813   nusach\n",
      "814   latin america\n",
      "815   2002\n",
      "816   conformity\n",
      "817   surname hail\n",
      "818   herodotus\n",
      "819   many jews\n",
      "820   first half\n",
      "821   nusach sefard\n",
      "822   estimated 8.8 million\n",
      "823   genesis rabbah\n",
      "824   religious jews\n",
      "825   uranus\n",
      "826   11 july neptune\n",
      "827   largest neptunian moon\n",
      "828   objects\n",
      "829   difference\n",
      "830   sun\n",
      "831   right\n",
      "832   result\n",
      "833   planet's distance\n",
      "834   fourth-largest planet\n",
      "835   rahab\n",
      "836   troposphere\n",
      "837   2007\n",
      "838   jupiter\n",
      "839   modern greek\n",
      "840   uranus\n",
      "841   johann galle\n",
      "842   same way\n",
      "843   william lassell\n",
      "844   late 2020s or early 2030s\n",
      "845   11 july 2011\n",
      "846   gravity\n",
      "847   le verrier\n",
      "848   neptune's composition\n",
      "849   200\n",
      "850   °\n",
      "851   , persistent methane clouds\n",
      "852   scooter\n",
      "853   small dark spot\n",
      "854   voyager\n",
      "855   high-altitude clouds\n",
      "856   voyager\n",
      "857   1:1 resonance\n",
      "858   weather\n",
      "859   1843\n",
      "860   winds\n",
      "861   occultation\n",
      "862   50 %\n",
      "863   focus\n",
      "864   advent\n",
      "865   25 august 1989\n",
      "866   credit\n",
      "867   discovery\n",
      "868   1821\n",
      "869   0.25 %\n",
      "870   roughly 40\n",
      "871   upper-level clouds\n",
      "872   sun\n",
      "873   first and so far only object\n",
      "874   dark spots\n",
      "875   adams\n",
      "876   17\n",
      "877   le verrier ring\n",
      "878   discovery\n",
      "879   neptune\n",
      "880   current most widely accepted explanation\n",
      "881   nahuatl\n",
      "882   2019\n",
      "883   1:1\n",
      "884   penultimate known planet\n",
      "885   uranus\n",
      "886   neptune\n",
      "887   2006\n",
      "888   23\n",
      "889   high-altitude clouds\n",
      "890   first and so far only object\n",
      "891   14\n",
      "892   jupiter\n",
      "893   most heavily populated resonance\n",
      "894   le verrier ring\n",
      "895   sun\n",
      "896   voyager 2's arrival\n",
      "897   high-altitude cloud bands\n",
      "898   neptune's atmospheric methane content\n",
      "899   neptune\n",
      "900   discovery\n",
      "901   evening\n",
      "902   december 1612\n",
      "903   altitudes\n",
      "904   vortex structures\n",
      "905   neptune's more varied weather\n",
      "906   30.1\n",
      "907   international astronomical union\n",
      "908   sun\n",
      "909   current models\n",
      "910   1997\n",
      "911   planetary science\n",
      "912   400 m/s\n",
      "913   1981\n",
      "914   images\n",
      "915   1017\n",
      "916   candidates\n",
      "917   neptune's more varied weather\n",
      "918   reaction\n",
      "919   hobbes\n",
      "920   modern term `` culture\n",
      "921   non-material culture\n",
      "922   citation\n",
      "923   19th-century\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924   terror management theory\n",
      "925   29\n",
      "926   united states\n",
      "927   germany\n",
      "928   end of the last ice age\n",
      "929   19th-century critics\n",
      "930   contrasted\n",
      "931   terror management theory\n",
      "932   anthropologists\n",
      "933   1970s\n",
      "934   around 50,000 years ago\n",
      "935   united states\n",
      "936   diffusion\n",
      "937   prussian linguist\n",
      "938   1950s and 1960s\n",
      "939   stuart hall\n",
      "940   élite ideal\n",
      "941   present legislation\n",
      "942   jefferson's metaphor\n",
      "943   1971\n",
      "944   nonconformists\n",
      "945   areas\n",
      "946   pervasive secularism\n",
      "947   wall\n",
      "948   may 3, 2006\n",
      "949   still other scholars\n",
      "950   early as the mid-17th century\n",
      "951   religious freedom\n",
      "952   engel\n",
      "953   madison\n",
      "954   december 20, 2005\n",
      "955   william penn\n",
      "956   two\n",
      "957   legal scholars\n",
      "958   still other scholars\n",
      "959   reynolds\n",
      "960   1776\n",
      "961   court's decision\n",
      "962   opponents\n",
      "963   december\n",
      "964   robert s. wood\n",
      "965   u.s.\n",
      "966   1962\n",
      "967   court\n",
      "968   1994\n",
      "969   kurtzman\n",
      "970   argue\n",
      "971   thomas jefferson's influential virginia statute\n",
      "972   1947\n",
      "973   lone dissenter\n",
      "974   central point\n",
      "975   many early immigrant groups\n",
      "976   third\n",
      "977   may\n",
      "978   jefferson\n",
      "979   states' rights\n",
      "980   1879\n",
      "981   religious bigotry\n",
      "982   one religious society\n",
      "983   1786\n",
      "984   religious beliefs\n",
      "985   national motto\n",
      "986   u.s.\n",
      "987   ohio\n",
      "988   2002\n",
      "989   new england\n",
      "990   phrase\n",
      "991   three\n",
      "992   answer\n",
      "993   pledge\n",
      "994   principle or primary effect\n",
      "995   kurtzman\n",
      "996   john f. kennedy\n",
      "997   south carolina\n",
      "998   roger williams\n",
      "999   purpose of the display ( educating the public on american legal traditions ) was secular in nature .\n",
      "1000   december 27, 1657\n",
      "1001   jefferson's concept\n",
      "1002   jefferson's statute\n",
      "1003   eighteenth century\n",
      "1004   required beliefs\n",
      "1005   separation\n",
      "1006   consequence\n",
      "1007   1878\n",
      "1008   1835-1876\n",
      "1009   citation\n",
      "1010   proclamations\n",
      "1011   five-justice majority\n",
      "1012   2\n",
      "1013   epperson\n",
      "1014   roger williams\n",
      "1015   three\n",
      "1016   jefferson's metaphor\n",
      "1017   umayyad dynasty mosaic making\n",
      "1018   constantinople\n",
      "1019   venetian\n",
      "1020   one solsternus\n",
      "1021   earliest examples\n",
      "1022   fragments\n",
      "1023   end of the 5th century\n",
      "1024   justinian\n",
      "1025   alexander mosaic\n",
      "1026   5th-century\n",
      "1027   17th-century\n",
      "1028   grand prince\n",
      "1029   jerusalem\n",
      "1030   12th century\n",
      "1031   remains\n",
      "1032   whose sail\n",
      "1033   most prominent artist\n",
      "1034   greek masters\n",
      "1035   roman church\n",
      "1036   important justinian\n",
      "1037   mosaics\n",
      "1038   roger ii\n",
      "1039   double indirect method\n",
      "1040   another example\n",
      "1041   affricisco\n",
      "1042   hand\n",
      "1043   human figure\n",
      "1044   arta\n",
      "1045   decoration\n",
      "1046   abundant variety\n",
      "1047   monastic communities\n",
      "1048   5th-century\n",
      "1049   6th to the 15th centuries\n",
      "1050   8th–9th centuries\n",
      "1051   east\n",
      "1052   mary\n",
      "1053   themes\n",
      "1054   bethany\n",
      "1055   late 11th century\n",
      "1056   constantinople\n",
      "1057   southern italy\n",
      "1058   mausoleum\n",
      "1059   1169\n",
      "1060   piazza armerina\n",
      "1061   539\n",
      "1062   royal basilica\n",
      "1063   taddeo zuccari\n",
      "1064   sant'ambrogio\n",
      "1065   ascension\n",
      "1066   another samaritan synagogue\n",
      "1067   thessaloniki\n",
      "1068   normal technique\n",
      "1069   norman kings\n",
      "1070   mosaics\n",
      "1071   galla placidia\n",
      "1072   1310–14\n",
      "1073   iconoclastic\n",
      "1074   moorish spain\n",
      "1075   portugal\n",
      "1076   2003\n",
      "1077   seasons\n",
      "1078   church of the holy apostles\n",
      "1079   islamic architecture\n",
      "1080   raphael\n",
      "1081   1210\n",
      "1082   present-day\n",
      "1083   pebble mosaics\n",
      "1084   1894\n",
      "1085   nea moni monastery on chios\n",
      "1086   arabs\n",
      "1087   constantinople\n",
      "1088   1043–1056\n",
      "1089   10th-century\n",
      "1090   greek figural mosaics\n",
      "1091   rare example\n",
      "1092   striking technical innovation\n",
      "1093   12th-century\n",
      "1094   striking technical innovation\n",
      "1095   calçada portuguesa\n",
      "1096   river mureş\n",
      "1097   interesting fusion\n",
      "1098   santa constanza\n",
      "1099   damascus\n",
      "1100   5th-century\n",
      "1101   outstanding examples\n",
      "1102   styles\n",
      "1103   greek inscriptions\n",
      "1104   only surviving 12th-century mosaic work\n",
      "1105   atrium\n",
      "1106   second\n",
      "1107   large villa rustica\n",
      "1108   works\n",
      "1109   1971–72\n",
      "1110   1607\n",
      "1111   around 1143\n",
      "1112   classe\n",
      "1113   most prominent artist\n",
      "1114   abundant variety\n",
      "1115   kastron mefaa\n",
      "1116   constantinople\n",
      "1117   middle ages\n",
      "1118   symbolism\n",
      "1119   santa maria\n",
      "1120   abbot\n",
      "1121   julii\n",
      "1122   venetian\n",
      "1123   1913\n",
      "1124   royal basilica\n",
      "1125   damascus\n",
      "1126   middle\n",
      "1127   1310–14\n",
      "1128   emperor maximian\n",
      "1129   outstanding examples\n",
      "1130   10\n",
      "1131   18th century\n",
      "1132   hand\n",
      "1133   religious umayyad mosaic works\n",
      "1134   constantine monomachos\n",
      "1135   lebanon\n",
      "1136   synagogue\n",
      "1137   18th-century\n",
      "1138   accidents\n",
      "1139   1986\n",
      "1140   late 4th century\n",
      "1141   three\n",
      "1142   cathedral\n",
      "1143   figures\n",
      "1144   monastery of martyrius\n",
      "1145   christian\n",
      "1146   sosus\n",
      "1147   constantine monomachos\n",
      "1148   roger ii\n",
      "1149   saint victor\n",
      "1150   1589\n",
      "1151   constantine monomachos\n",
      "1152   venetian craftsmen\n",
      "1153   limoges\n",
      "1154   5th-century\n",
      "1155   iconoclastic era\n",
      "1156   1930\n",
      "1157   fragments\n",
      "1158   double indirect method\n",
      "1159   17th–19th centuries\n",
      "1160   exotic or mythological animals\n",
      "1161   5th-century ravenna\n",
      "1162   1960s\n",
      "1163   two-tone stone mosaic paving\n",
      "1164   64 ad\n",
      "1165   gelati\n",
      "1166   calçada portuguesa\n",
      "1167   constantinople\n",
      "1168   golden gate\n",
      "1169   4th-century bc\n",
      "1170   1043–1056\n",
      "1171   early 4th century\n",
      "1172   north africa\n",
      "1173   taungoo\n",
      "1174   80\n",
      "1175   official united states policy\n",
      "1176   capital city\n",
      "1177   these names\n",
      "1178   hanthawaddy monarchs\n",
      "1179   2008 rice production\n",
      "1180   name\n",
      "1181   obama\n",
      "1182   estimated 200,000\n",
      "1183   october\n",
      "1184   early civilisations\n",
      "1185   myanmar\n",
      "1186   first\n",
      "1187   myanmar's ethnic minority groups\n",
      "1188   little known fact\n",
      "1189   these names\n",
      "1190   thousands\n",
      "1191   10%\n",
      "1192   largest empire\n",
      "1193   1,930 km\n",
      "1194   us\n",
      "1195   myanmar\n",
      "1196   myanmar\n",
      "1197   majority burmese bamar ethnic group\n",
      "1198   west\n",
      "1199   name\n",
      "1200   burma's education system\n",
      "1201   august\n",
      "1202   colonial architectural influences\n",
      "1203   2008\n",
      "1204   official united states policy\n",
      "1205   monks\n",
      "1206   400,000 years ago\n",
      "1207   1770\n",
      "1208   time\n",
      "1209   1,700\n",
      "1210   400,000 years ago\n",
      "1211   name\n",
      "1212   myanmar\n",
      "1213   major battleground\n",
      "1214   250 years\n",
      "1215   hanthawaddy monarchs\n",
      "1216   lines\n",
      "1217   1883\n",
      "1218   equitable right\n",
      "1219   civil law\n",
      "1220   uk\n",
      "1221   criticism\n",
      "1222   lines\n",
      "1223   2004\n",
      "1224   estimated 18 million\n",
      "1225   claim\n",
      "1226   napster\n",
      "1227   law professor\n",
      "1228   one year's\n",
      "1229   free software foundation founder richard stallman\n",
      "1230   free software foundation founder richard stallman\n",
      "1231   scope\n",
      "1232   many countries\n",
      "1233   definition\n",
      "1234   scope\n",
      "1235   patent infringement\n",
      "1236   copyleft\n",
      "1237   united states\n",
      "1238   1994\n",
      "1239   citation\n",
      "1240   1846\n",
      "1241   many jurisdictions\n",
      "1242   arguments\n",
      "1243   definition\n",
      "1244   three\n",
      "1245   term\n",
      "1246   infringement\n",
      "1247   specific technological problem\n",
      "1248   lines\n",
      "1249   1883\n",
      "1250   trips\n",
      "1251   copyright\n",
      "1252   limitations\n",
      "1253   recording industry association\n",
      "1254   referring\n",
      "1255   stephan kinsella\n",
      "1256   united states\n",
      "1257   context\n",
      "1258   equitable right\n",
      "1259   context\n",
      "1260   recent developments\n",
      "1261   1791\n",
      "1262   may 2011\n",
      "1263   1998\n",
      "1264   trade secret misappropriation\n",
      "1265   2001\n",
      "1266   october 1845\n",
      "1267   civil law\n",
      "1268   1883\n",
      "1269   one year's\n",
      "1270   critics\n",
      "1271   17 the stated objective\n",
      "1272   motion picture association of america\n",
      "1273   uk\n",
      "1274   trademark infringement\n",
      "1275   millions\n",
      "1276   high-tech fields\n",
      "1277   native cultures\n",
      "1278   critics\n",
      "1279   intellectual-property rights\n",
      "1280   native cultures\n",
      "1281   berne\n",
      "1282   detriment\n",
      "1283   infringement\n",
      "1284   assumption\n",
      "1285   \n",
      "1286   french\n",
      "1287   rossiya\n",
      "1288   ingur river\n",
      "1289   1830\n",
      "1290   kars\n",
      "1291   christians\n",
      "1292   ottoman empire\n",
      "1293   crimean war\n",
      "1294   1854\n",
      "1295   time\n",
      "1296   constantinople\n",
      "1297   black sea\n",
      "1298   danube\n",
      "1299   july 1854\n",
      "1300   26 july\n",
      "1301   omar pasha\n",
      "1302   north\n",
      "1303   russians\n",
      "1304   14 april\n",
      "1305   russia\n",
      "1306   fewer than half\n",
      "1307   spring\n",
      "1308   local commanders\n",
      "1309   hill\n",
      "1310   august 1855\n",
      "1311   nicholas\n",
      "1312   15,000\n",
      "1313   four\n",
      "1314   russians\n",
      "1315   sides\n",
      "1316   william howard russell\n",
      "1317   time\n",
      "1318   1857\n",
      "1319   25 september\n",
      "1320   lord cowley\n",
      "1321   war\n",
      "1322   february\n",
      "1323   morning\n",
      "1324   piedmont\n",
      "1325   nicholas\n",
      "1326   late september\n",
      "1327   year\n",
      "1328   three\n",
      "1329   bulgaria\n",
      "1330   alexander ii\n",
      "1331   french\n",
      "1332   cronstadt\n",
      "1333   second\n",
      "1334   28 march\n",
      "1335   12 july\n",
      "1336   339\n",
      "1337   august 1855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1338   black sea\n",
      "1339   31 december\n",
      "1340   cronstadt\n",
      "1341   george hamilton-gordon\n",
      "1342   5 september\n",
      "1343   32–40\n",
      "1344   alliance\n",
      "1345   local commanders\n",
      "1346   russians\n",
      "1347   crimean war\n",
      "1348   movement\n",
      "1349   omar pasha\n",
      "1350   nicholas\n",
      "1351   parliament\n",
      "1352   constantinople\n",
      "1353   åland islands\n",
      "1354   alexander ii\n",
      "1355   nicholas\n",
      "1356   cardigan\n",
      "1357   treaty of paris\n",
      "1358   austria\n",
      "1359   start\n",
      "1360   october 1853\n",
      "1361   french\n",
      "1362   sunday\n",
      "1363   centuries-old\n",
      "1364   public opinion\n",
      "1365   second counterattack\n",
      "1366   winter of 1854\n",
      "1367   sinop\n",
      "1368   russian cavalry movement\n",
      "1369   catholic support\n",
      "1370   peaceful settlement\n",
      "1371   1830\n",
      "1372   corps\n",
      "1373   vidin\n",
      "1374   reaction\n",
      "1375   september 1853\n",
      "1376   danube river\n",
      "1377   ottoman forces\n",
      "1378   william howard russell\n",
      "1379   june 24, 1839\n",
      "1380   local commanders\n",
      "1381   105  the tsar\n",
      "1382   28 march 1854\n",
      "1383   432–33\n",
      "1384   roger fenton\n",
      "1385   russian troops\n",
      "1386   far south wrangel\n",
      "1387   august\n",
      "1388   461 the resulting battle\n",
      "1389   14 november\n",
      "1390   alfred lord tennyson\n",
      "1391   however\n",
      "1392   sultan\n",
      "1393   russians\n",
      "1394   439 an electrical telegraph\n",
      "1395   victor emmanuel ii\n",
      "1396   september 1854\n",
      "1397   200 years\n",
      "1398   principalities\n",
      "1399   black sea\n",
      "1400   vidin\n",
      "1401   russians\n",
      "1402   may\n",
      "1403   calafat\n",
      "1404   time\n",
      "1405   cardigan\n",
      "1406   19 october\n",
      "1407   little additional naval action\n",
      "1408   london\n",
      "1409   marani\n",
      "1410   3,300\n",
      "1411   sevastopol\n",
      "1412   local commanders\n",
      "1413   february\n",
      "1414   chetatea\n",
      "1415   time\n",
      "1416   omar pasha\n",
      "1417   nicholas\n",
      "1418   1840\n",
      "1419   balkans\n",
      "1420   times newspaper\n",
      "1421   a. w\n",
      "1422   danube\n",
      "1423   note\n",
      "1424   large body\n",
      "1425   snow\n",
      "1426   march 1854\n",
      "1427   nicholas\n",
      "1428   azov sea\n",
      "1429   sunday\n",
      "1430   taganrog\n",
      "1431   8000\n",
      "1432   april\n",
      "1433   omar pasha\n",
      "1434   5 september\n",
      "1435   uk\n",
      "1436   february\n",
      "1437   ottoman army\n",
      "1438   russians\n",
      "1439   411 nicholas\n",
      "1440   russian america\n",
      "1441   5 december 1853\n",
      "1442   28 may\n",
      "1443   aberdeen\n",
      "1444   however\n",
      "1445   kerch strait\n",
      "1446   192\n",
      "1447   8000\n",
      "1448   september 8000\n",
      "1449   1848\n",
      "1450   february 1853\n",
      "1451   april 1854\n",
      "1452   201 the ships\n",
      "1453   alaska\n",
      "1454   30\n",
      "1455   seven columns\n",
      "1456   april 1854 to february 1857\n",
      "1457   :104:19\n",
      "1458   crimean war\n",
      "1459   failure\n",
      "1460   30 march 1856\n",
      "1461   peace terms\n",
      "1462   russians\n",
      "1463   19 october\n",
      "1464   black sea\n",
      "1465   greeks\n",
      "1466   20000\n",
      "1467   menshikov\n",
      "1468   war\n",
      "1469   west\n",
      "1470   battle of sinop\n",
      "1471   late july 1854\n",
      "1472   8000\n",
      "1473   september 1854\n",
      "1474   first\n",
      "1475   october 1853\n",
      "1476   malakoff\n",
      "1477   chetatea\n",
      "1478   1830\n",
      "1479   `` clerical party\n",
      "1480   civilian engineering crew\n",
      "1481   30 march 1856\n",
      "1482   russia\n",
      "1483   30000\n",
      "1484   sevastopol\n",
      "1485   paris\n",
      "1486   popularisation\n",
      "1487   little advance\n",
      "1488   war\n",
      "1489   otto von bismarck\n",
      "1490   industrial explosives\n",
      "1491   british\n",
      "1492   1830\n",
      "1493   chetatea\n",
      "1494   february\n",
      "1495   telegraph\n",
      "1496   25 october\n",
      "1497   another seventeen years\n",
      "1498   emperors\n",
      "1499   1720s\n",
      "1500   manchus\n",
      "1501   yongzheng\n",
      "1502   1623\n",
      "1503   pro-japanese koreans\n",
      "1504   name\n",
      "1505   sixty-one year\n",
      "1506   1750\n",
      "1507   france\n",
      "1508   sun yat-sen\n",
      "1509   qing emperors\n",
      "1510   eight\n",
      "1511   qing dynasty\n",
      "1512   1900\n",
      "1513   ming dynasty\n",
      "1514   beijing\n",
      "1515   yongzheng\n",
      "1516   diplomatic and strategic problems\n",
      "1517   instead\n",
      "1518   name\n",
      "1519   qing armies\n",
      "1520   first opium war\n",
      "1521   yuan shikai\n",
      "1522   hooge\n",
      "1523   3,400\n",
      "1524   1905\n",
      "1525   peasant rebels\n",
      "1526   britain\n",
      "1527   qing emperors\n",
      "1528   kangxi emperor\n",
      "1529   xiang army\n",
      "1530   1641\n",
      "1531   1850s\n",
      "1532   team\n",
      "1533   zhu zhiliang\n",
      "1534   shunzhi emperor\n",
      "1535   two\n",
      "1536   citation\n",
      "1537   zeng guofan\n",
      "1538   watershed\n",
      "1539   korea\n",
      "1540   kangxi emperor\n",
      "1541   another government institution\n",
      "1542   mid-19th century\n",
      "1543   1661\n",
      "1544   many han\n",
      "1545   1683\n",
      "1546   fall\n",
      "1547   zeng guofan's strategy\n",
      "1548   20 million\n",
      "1549   li\n",
      "1550   18th century\n",
      "1551   manchus\n",
      "1552   chinese\n",
      "1553   late ming dynasty\n",
      "1554   1641\n",
      "1555   manchus\n",
      "1556   qing organization\n",
      "1557   yuan shikai\n",
      "1558   \n",
      "1559   china\n",
      "1560   1860\n",
      "1561   puyi\n",
      "1562   ming general wu sangui\n",
      "1563   yuan shikai\n",
      "1564   taiwan\n",
      "1565   chongzhen emperor\n",
      "1566   ten great campaigns\n",
      "1567   18th century\n",
      "1568   1930s\n",
      "1569   department\n",
      "1570   liaodong\n",
      "1571   guangxu\n",
      "1572   hong taiji\n",
      "1573   sixty-one year reign\n",
      "1574   beijing\n",
      "1575   \n",
      "1576   hong xiuquan\n",
      "1577   han chinese\n",
      "1578   xinjiang\n",
      "1579   british\n",
      "1580   400,000\n",
      "1581   daoguang emperor\n",
      "1582   qinghai\n",
      "1583   ming dynasty\n",
      "1584   1930s\n",
      "1585   yingzhen\n",
      "1586   first sino-japanese war\n",
      "1587   western powers\n",
      "1588   calligraphy\n",
      "1589   units\n",
      "1590   jiangnan\n",
      "1591   qing surrender\n",
      "1592   manchu generals\n",
      "1593   western powers\n",
      "1594   name\n",
      "1595   several ming princes\n",
      "1596   53\n",
      "1597   board\n",
      "1598   koxinga\n",
      "1599   eight years\n",
      "1600   promises\n",
      "1601   china\n",
      "1602   qing\n",
      "1603   1850s\n",
      "1604   xinjiang\n",
      "1605   1661\n",
      "1606   day\n",
      "1607   qianlong emperor\n",
      "1608   yuan\n",
      "1609   turnover\n",
      "1610   400,000\n",
      "1611   1861 and 1873\n",
      "1612   qing emperors\n",
      "1613   early manchu rulers\n",
      "1614   qing court\n",
      "1615   most significant fact\n",
      "1616   beijing\n",
      "1617   1661\n",
      "1618   1858\n",
      "1619   han bannermen\n",
      "1620   eight regional viceroys\n",
      "1621   18\n",
      "1622   majority\n",
      "1623   ming dynasty\n",
      "1624   jiading\n",
      "1625   17\n",
      "1626   cabinet\n",
      "1627   1681\n",
      "1628   1683\n",
      "1629   1711\n",
      "1630   yingzhen\n",
      "1631   ci'an\n",
      "1632   november 14, 1908\n",
      "1633   nine\n",
      "1634   fulin\n",
      "1635   dungan revolt\n",
      "1636   qīng cháo\n",
      "1637   qianlong emperor\n",
      "1638   years\n",
      "1639   manchus\n",
      "1640   1673\n",
      "1641   literature\n",
      "1642   2,000 years\n",
      "1643   80 %\n",
      "1644   first opium war\n",
      "1645   november 14, 1908\n",
      "1646   april 1644\n",
      "1647   yaqub beg\n",
      "1648   september 1643\n",
      "1649   li zicheng\n",
      "1650   yongying system\n",
      "1651   imperial palace\n",
      "1652   1673-1674\n",
      "1653   november 14, 1908\n",
      "1654   han bannermen\n",
      "1655   16 %\n",
      "1656   art\n",
      "1657   yongzheng\n",
      "1658   india\n",
      "1659   1854\n",
      "1660   far\n",
      "1661   1889\n",
      "1662   remaining banners\n",
      "1663   yingzhen\n",
      "1664   qianlong emperor's reign\n",
      "1665   christianity\n",
      "1666   3,400\n",
      "1667   five\n",
      "1668   qing\n",
      "1669   qing\n",
      "1670   shang kexi\n",
      "1671   remaining banners\n",
      "1672   another seventeen years\n",
      "1673   multi-ethnic force\n",
      "1674   manchus\n",
      "1675   xinjiang\n",
      "1676   terms\n",
      "1677   1683\n",
      "1678   beijing\n",
      "1679   ming han chinese armies\n",
      "1680   qing\n",
      "1681   qing\n",
      "1682   suiyuan shidan\n",
      "1683   qing dynasty\n",
      "1684   1782\n",
      "1685   1637\n",
      "1686   full control\n",
      "1687   qing dynasty\n",
      "1688   nurhaci\n",
      "1689   ming han chinese armies\n",
      "1690   first sino-japanese war\n",
      "1691   addition\n",
      "1692   emperors\n",
      "1693   1641\n",
      "1694   dorgon's controversial july 1645 edict\n",
      "1695   grand secretariat , [ f\n",
      "1696   november\n",
      "1697   units\n",
      "1698   early manchu rulers\n",
      "1699   qīng cháo\n",
      "1700   pro-japanese koreans\n",
      "1701   1870\n",
      "1702   yaqub beg\n",
      "1703   wu\n",
      "1704   short story form\n",
      "1705   units\n",
      "1706   manchuria\n",
      "1707   eight years old\n",
      "1708   guangxu\n",
      "1709   few ethnic manchus\n",
      "1710   china\n",
      "1711   remaining banners\n",
      "1712   17\n",
      "1713   early manchu rulers\n",
      "1714   november 14, 1908\n",
      "1715   ningguta\n",
      "1716   chief\n",
      "1717   jiangyin\n",
      "1718   1723\n",
      "1719   guangxu\n",
      "1720   1727\n",
      "1721   1860\n",
      "1722   november 1, 1897\n",
      "1723   qīng cháo\n",
      "1724   qianlong emperor\n",
      "1725   october 30\n",
      "1726   suiyuan shidan\n",
      "1727   two\n",
      "1728   dst\n",
      "1729   evening\n",
      "1730   opposition\n",
      "1731   continental europe\n",
      "1732   canadian\n",
      "1733   time\n",
      "1734   next year\n",
      "1735   clocks\n",
      "1736   iceland\n",
      "1737   years\n",
      "1738   daylight\n",
      "1739   three-year trial\n",
      "1740   example\n",
      "1741   1895\n",
      "1742   2008\n",
      "1743   groups\n",
      "1744   us\n",
      "1745   normal dst\n",
      "1746   \n",
      "1747   54.5 %\n",
      "1748   summer\n",
      "1749   forms\n",
      "1750   argue\n",
      "1751   2011 to 2014\n",
      "1752   tz=':america/new_york'\n",
      "1753   france\n",
      "1754   standardize\n",
      "1755   north american\n",
      "1756   1993\n",
      "1757   3.5 %\n",
      "1758   2008 doe report\n",
      "1759   july\n",
      "1760   october 5\n",
      "1761   early proponents\n",
      "1762   retinitis pigmentosa foundation fighting blindness\n",
      "1763   willett's 1907 proposal\n",
      "1764   1907\n",
      "1765   3.5 %\n",
      "1766   dst\n",
      "1767   2005\n",
      "1768   2005\n",
      "1769   keeps\n",
      "1770   xinjiang\n",
      "1771   1895\n",
      "1772   new zealand\n",
      "1773   merriam-webster\n",
      "1774   regions\n",
      "1775   united kingdom\n",
      "1776   sunlight exposure\n",
      "1777   unexpected adverse effect\n",
      "1778   winston churchill\n",
      "1779   computer software\n",
      "1780   proposal\n",
      "1781   1907\n",
      "1782   1995\n",
      "1783   microsoft windows\n",
      "1784   negative dst\n",
      "1785   britain\n",
      "1786   standard time\n",
      "1787   autumn and two hours\n",
      "1788   merriam-webster's\n",
      "1789   two hours\n",
      "1790   dst\n",
      "1791   correlation\n",
      "1792   (cet\n",
      "1793   years\n",
      "1794   fixed work schedules\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795   example\n",
      "1796   dst\n",
      "1797   november\n",
      "1798   reduces\n",
      "1799   1970s\n",
      "1800   older form\n",
      "1801   march 2011\n",
      "1802   2005\n",
      "1803   us\n",
      "1804   winter\n",
      "1805   north america\n",
      "1806   1895\n",
      "1807   ntfs\n",
      "1808   \n",
      "1809   northern summer\n",
      "1810   farmers ' groups\n",
      "1811   countries\n",
      "1812   dst inherits\n",
      "1813   dst\n",
      "1814   filesystem\n",
      "1815   2007\n",
      "1816   merriam-webster\n",
      "1817   farmers ' groups\n",
      "1818   st\n",
      "1819   people\n",
      "1820   time zone differences\n",
      "1821   early goal\n",
      "1822   one reason\n",
      "1823   daylight\n",
      "1824   regions\n",
      "1825   kingsford charcoal\n",
      "1826   1999 study\n",
      "1827   many enactments\n",
      "1828   one\n",
      "1829   year-independent way\n",
      "1830   autumn and two hours\n",
      "1831   least two\n",
      "1832   us\n",
      "1833   clocks\n",
      "1834   iceland\n",
      "1835   europe\n",
      "1836   britain\n",
      "1837   2000\n",
      "1838   daily\n",
      "1839   name\n",
      "1840   clocks\n",
      "1841   united kingdom\n",
      "1842   franklin's 1784 satire\n",
      "1843   daylight\n",
      "1844   electricity use\n",
      "1845   times\n",
      "1846   1984\n",
      "1847   adopt\n",
      "1848   britain\n",
      "1849   dst\n",
      "1850   first three weekdays\n",
      "1851   plan\n",
      "1852   standardize\n",
      "1853   14 april\n",
      "1854   2008\n",
      "1855   people\n",
      "1856   us\n",
      "1857   summer hours\n",
      "1858   little impact\n",
      "1859   accidents\n",
      "1860   example\n",
      "1861   summer\n",
      "1862   taxing shutters\n",
      "1863   hyphenated\n",
      "1864   andrew peters\n",
      "1865   dst\n",
      "1866   much as 11 %\n",
      "1867   day\n",
      "1868   similar twice-yearly tasks\n",
      "1869   2005\n",
      "1870   dairy farmers\n",
      "1871   modern dst\n",
      "1872   months of the year\n",
      "1873   1996\n",
      "1874   file\n",
      "1875   1987–2006\n",
      "1876   2007 us change\n",
      "1877   britain\n",
      "1878   may\n",
      "1879   repeals\n",
      "1880   may 1965\n",
      "1881   annual\n",
      "1882   2005\n",
      "1883   1983\n",
      "1884   los angeles\n",
      "1885   dick dale\n",
      "1886   end of the decade\n",
      "1887   scottish\n",
      "1888   2008\n",
      "1889   1980s\n",
      "1890   united states\n",
      "1891   canadian\n",
      "1892   1958\n",
      "1893   elements\n",
      "1894   led zeppelin\n",
      "1895   double platinum ballbreaker\n",
      "1896   bigger hit\n",
      "1897   bon scott\n",
      "1898   one\n",
      "1899   1980s\n",
      "1900   breakup\n",
      "1901   irish\n",
      "1902   david lee roth\n",
      "1903   black crowes\n",
      "1904   poison\n",
      "1905   aerosmith's comeback\n",
      "1906   los angeles music scene\n",
      "1907   bon scott\n",
      "1908   fellow australians airbourne's début album\n",
      "1909   alice cooper\n",
      "1910   los angeles\n",
      "1911   early forms\n",
      "1912   rock anthem\n",
      "1913   2009\n",
      "1914   san francisco\n",
      "1915   hard rock roots\n",
      "1916   bass guitar\n",
      "1917   1993\n",
      "1918   1980s\n",
      "1919   1980s\n",
      "1920   black sabbath's\n",
      "1921   us\n",
      "1922   guns n' roses\n",
      "1923   ramones\n",
      "1924   bass guitar\n",
      "1925   bon jovi\n",
      "1926   heavy metal\n",
      "1927   number\n",
      "1928   audioslave\n",
      "1929   1992\n",
      "1930   david lee roth\n",
      "1931   four\n",
      "1932   1981\n",
      "1933   1987\n",
      "1934   new wave\n",
      "1935   term\n",
      "1936   bon jovi's 1995 album\n",
      "1937   bon jovi's third album\n",
      "1938   kinks\n",
      "1939   others\n",
      "1940   ac/dc\n",
      "1941   guns n' roses\n",
      "1942   first commercial isp\n",
      "1943   transit isps\n",
      "1944   13 april 2015\n",
      "1945   fcc chairman\n",
      "1946   virtual isp ( visp\n",
      "1947   services\n",
      "1948   possible solution\n",
      "1949   4 years\n",
      "1950   1995\n",
      "1951   citation\n",
      "1952   intelligence agencies\n",
      "1953   customers\n",
      "1954   mailbox provider\n",
      "1955   internet\n",
      "1956   fcc chairman\n",
      "1957   free isps\n",
      "1958   isps\n",
      "1959   isps\n",
      "1960   isps\n",
      "1961   upstream isp\n",
      "1962   prism\n",
      "1963   many mailbox providers\n",
      "1964   same concept\n",
      "1965   26 february 2015\n",
      "1966   transit isps\n",
      "1967   free isps\n",
      "1968   services\n",
      "1969   university of kansas school of business\n",
      "1970   three\n",
      "1971   top law school\n",
      "1972   university\n",
      "1973   lawrence\n",
      "1974   school's sports teams\n",
      "1975   university of kansas school of business\n",
      "1976   one\n",
      "1977   overland park\n",
      "1978   march 21, 1865\n",
      "1979   1939\n",
      "1980   university of kansas school\n",
      "1981   community tool box\n",
      "1982   university daily kansan\n",
      "1983   ku football\n",
      "1984   wichita\n",
      "1985   2,100\n",
      "1986   kansas union\n",
      "1987   wichita\n",
      "1988   j-school's students\n",
      "1989   four year\n",
      "1990   first\n",
      "1991   kansas public radio station kanu\n",
      "1992   2,100\n",
      "1993   university of kansas school of business\n",
      "1994   1890\n",
      "1995   number\n",
      "1996   ku school of engineering\n",
      "1997   kansas\n",
      "1998   thirteen\n",
      "1999   university\n",
      "2000   university of kansas school of business\n",
      "2001   kansas union\n",
      "2002   bs/ms\n",
      "2003   university of kansas school of business\n",
      "2004   ku men's basketball team\n",
      "2005   university\n",
      "2006   wichita\n",
      "2007   medical center\n",
      "2008   first union\n",
      "2009   princeton review\n",
      "2010   designintelligence\n",
      "2011   watson library\n",
      "2012   48 months\n",
      "2013   princeton review\n",
      "2014   one\n",
      "2015   university of kansas school of architecture , design\n",
      "2016   2,100\n",
      "2017   university of kansas school of business\n",
      "2018   america’s first foundation\n",
      "2019   university of kansas school of business\n",
      "2020   university\n",
      "2021   daily\n",
      "2022   medical center and university hospital\n",
      "2023   mark mangino\n",
      "2024   ku men's basketball team\n",
      "2025   journal\n",
      "2026   emporia\n",
      "2027   ku's academic computing department\n",
      "2028   university of kansas school of business\n",
      "2029   team\n",
      "2030   university of kansas school of business\n",
      "2031   1891\n",
      "2032   educational and research sites\n",
      "2033   ku cross country\n",
      "2034   citation\n",
      "2035   additional funds\n",
      "2036   university daily kansan\n",
      "2037   23,597\n",
      "2038   school newspaper\n",
      "2039   most recent championship\n",
      "2040   university of kansas school\n",
      "2041   1891\n",
      "2042   ku department\n",
      "2043   thirteen\n",
      "2044   1891\n",
      "2045   2007\n",
      "2046   w 15th st\n",
      "2047   enrollment\n",
      "2048   1952\n",
      "2049   university\n",
      "2050   sheahon zenger\n",
      "2051   ku's school of business\n",
      "2052   university of kansas school of business\n",
      "2053   than 345\n",
      "2054   free , online resource\n",
      "2055   kansas\n",
      "2056   5\n",
      "2057   university\n",
      "2058   ku school of engineering\n",
      "2059   university of kansas school of law\n",
      "2060   2\n",
      "2061   western europe\n",
      "2062   jana bennett\n",
      "2063   1974\n",
      "2064   bbc one\n",
      "2065   dedicated tv channel\n",
      "2066   bbc television\n",
      "2067   service\n",
      "2068   bbc one\n",
      "2069   16\n",
      "2070   bbc\n",
      "2071   bbc\n",
      "2072   broadcasting act 1990\n",
      "2073   weeks\n",
      "2074   bbc domestic television channels\n",
      "2075   1974\n",
      "2076   welsh\n",
      "2077   queen elizabeth\n",
      "2078   world war ii\n",
      "2079   basement\n",
      "2080   bbc television\n",
      "2081   britain\n",
      "2082   english\n",
      "2083   uk\n",
      "2084   2000\n",
      "2085   britain\n",
      "2086   bbc television service\n",
      "2087   other uk nations\n",
      "2088   bbc\n",
      "2089   1967\n",
      "2090   alexandra palace\n",
      "2091   programmes\n",
      "2092   bbc television service\n",
      "2093   few years\n",
      "2094   studio\n",
      "2095   nearly 1000\n",
      "2096   pioneering bbc television series\n",
      "2097   drama\n",
      "2098   ceefax\n",
      "2099   september 1939\n",
      "2100   1927\n",
      "2101   30\n",
      "2102   commissioning\n",
      "2103   bbc television\n",
      "2104   december 2004\n",
      "2105   different remit\n",
      "2106   bbc television\n",
      "2107   existing vhf 405-line system\n",
      "2108   1967\n",
      "2109   bbc television department\n",
      "2110   daily\n",
      "2111   mechanical camera\n",
      "2112   bbc\n",
      "2113   larger quarters\n",
      "2114   december 2004\n",
      "2115   queen elizabeth\n",
      "2116   £85 million\n",
      "2117   london\n",
      "2118   television service's technical staff\n",
      "2119   [original research\n",
      "2120   dvd\n",
      "2121   david attenborough\n",
      "2122   series\n",
      "2123   june 1932\n",
      "2124   united states\n",
      "2125   2008\n",
      "2126   division\n",
      "2127   united kingdom\n",
      "2128   estimated 25,000–40,000\n",
      "2129   bbc television\n",
      "2130   london\n",
      "2131   weeks\n",
      "2132   division\n",
      "2133   february\n",
      "2134   1967 tom and jerry cartoons\n",
      "2135   bbc tv\n",
      "2136   half-century\n",
      "2137   1950s\n",
      "2138   625-line\n",
      "2139   5 july 2004\n",
      "2140   1964\n",
      "2141   1927\n",
      "2142   general entertainment channel\n",
      "2143   uk\n",
      "2144   bbc scotland's\n",
      "2145   1967\n",
      "2146   dvd\n",
      "2147   genetic groups\n",
      "2148   rise\n",
      "2149   united states\n",
      "2150   survey\n",
      "2151   empirical and conceptual problems\n",
      "2152   one crucial innovation\n",
      "2153   practice\n",
      "2154   93 %\n",
      "2155   michelle alexander\n",
      "2156   others\n",
      "2157   health disparities\n",
      "2158   fuzzy sets\n",
      "2159   united states\n",
      "2160   races\n",
      "2161   individual\n",
      "2162   abu el-haj\n",
      "2163   common misconception\n",
      "2164   physical anthropologists\n",
      "2165   early human genetic cluster analysis studies\n",
      "2166   races\n",
      "2167   early 20th century\n",
      "2168   human populations\n",
      "2169   societies\n",
      "2170   authors\n",
      "2171   race\n",
      "2172   kittles\n",
      "2173   result\n",
      "2174   early human genetic cluster analysis studies\n",
      "2175   federal government policy\n",
      "2176   socioeconomic factors\n",
      "2177   large geographic distances\n",
      "2178   42.4%\n",
      "2179   brutal conflicts\n",
      "2180   increasing number\n",
      "2181   brazil\n",
      "2182   seven\n",
      "2183   whites\n",
      "2184   categories\n",
      "2185   biological race concept\n",
      "2186   population\n",
      "2187   existence\n",
      "2188   scientific classification\n",
      "2189   subspecies\n",
      "2190   sex\n",
      "2191   rigid descent rule\n",
      "2192   forensic physical anthropologist\n",
      "2193   1949\n",
      "2194   skeletal remain\n",
      "2195   least two\n",
      "2196   sense\n",
      "2197   falsify\n",
      "2198   recent work\n",
      "2199   generally accepted concept\n",
      "2200   greatest number\n",
      "2201   many\n",
      "2202   survey\n",
      "2203   races\n",
      "2204   30 %\n",
      "2205   clustering\n",
      "2206   today\n",
      "2207   today\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2208   2000\n",
      "2209   brazilian child\n",
      "2210   racism\n",
      "2211   suggest\n",
      "2212   populations\n",
      "2213   cladistics\n",
      "2214   phylogenetic analysis\n",
      "2215   initial hypotheses\n",
      "2216   east asians\n",
      "2217   populations\n",
      "2218   practice\n",
      "2219   significant number\n",
      "2220   700.000\n",
      "2221   clade\n",
      "2222   diagnosis\n",
      "2223   concept\n",
      "2224   eduardo bonilla-silva\n",
      "2225   physical anthropologists\n",
      "2226   adversely\n",
      "2227   mass incarceration\n",
      "2228   many thousands\n",
      "2229   france\n",
      "2230   y chromosomes\n",
      "2231   5 %\n",
      "2232   roughly 28–37 %\n",
      "2233   93 %\n",
      "2234   uses\n",
      "2235   international epidemiological data\n",
      "2236   cranial measurements\n",
      "2237   european concept\n",
      "2238   arbitrary matter\n",
      "2239   1964\n",
      "2240   another way\n",
      "2241   blumenbach\n",
      "2242   good arguments\n",
      "2243   word\n",
      "2244   earlier work\n",
      "2245   researchers\n",
      "2246   last two decades\n",
      "2247   eduardo bonilla-silva\n",
      "2248   race\n",
      "2249   many people\n",
      "2250   2003 paper\n",
      "2251   system\n",
      "2252   five percent\n",
      "2253   europeans\n",
      "2254   kaplan\n",
      "2255   many biological anthropologists\n",
      "2256   amerindians\n",
      "2257   identification code\n",
      "2258   geographic or class criteria\n",
      "2259   use\n",
      "2260   five\n",
      "2261   thus, anthropologist frank livingstone's conclusion\n",
      "2262   values\n",
      "2263   study\n",
      "2264   abu el-haj\n",
      "2265   population\n",
      "2266   mitochondrial dna\n",
      "2267   sewall wright\n",
      "2268   way\n",
      "2269   1978\n",
      "2270   researchers\n",
      "2271   1775 treatise\n",
      "2272   markers\n",
      "2273   sense\n",
      "2274   questioning\n",
      "2275   three\n",
      "2276   anthropologists\n",
      "2277   one result\n",
      "2278   last decades\n",
      "2279   term race\n",
      "2280   william c. boyd\n",
      "2281   examples\n",
      "2282   empirical reasons\n",
      "2283   designation 'subspecies\n",
      "2284   brazilian child\n",
      "2285   early 20th century\n",
      "2286   cladistics\n",
      "2287   dean amadon\n",
      "2288   three\n",
      "2289   \n",
      "2290   contradictions\n",
      "2291   similarly\n",
      "2292   rise\n",
      "2293   jim crow laws\n",
      "2294   racial groups\n",
      "2295   balanced degree\n",
      "2296   blumenbach\n",
      "2297   concept\n",
      "2298   many others\n",
      "2299   dean amadon\n",
      "2300   european union\n",
      "2301   than 1.8 million years ago\n",
      "2302   eduardo bonilla-silva\n",
      "2303   60-65 %\n",
      "2304   example\n",
      "2305   fixation index\n",
      "2306   widespread popularity\n",
      "2307   racial groups\n",
      "2308   rather than 85%\n",
      "2309   anthropologist norman sauer\n",
      "2310   authors\n",
      "2311   thousands\n",
      "2312   19th century\n",
      "2313   2008\n",
      "2314   selected allele\n",
      "2315   20th century\n",
      "2316   scientists\n",
      "2317   anthropologists\n",
      "2318   \n",
      "2319   majority\n",
      "2320   non-african groups\n",
      "2321   last two decades of the 18th century\n",
      "2322   clade\n",
      "2323   racial groups\n",
      "2324   material\n",
      "2325   race\n",
      "2326   united states\n",
      "2327   selected allele\n",
      "2328   18th century\n",
      "2329   77\n",
      "2330   2000 edition\n",
      "2331   small non-representative sample\n",
      "2332   states\n",
      "2333   definitions\n",
      "2334   sociological factors\n",
      "2335   trained anthropologist\n",
      "2336   human populations\n",
      "2337   historically recent acceleration\n",
      "2338   mixture\n",
      "2339   racial discrimination\n",
      "2340   mammalian species\n",
      "2341   authors\n",
      "2342   graves\n",
      "2343   utility\n",
      "2344   loci\n",
      "2345   first post-classical published classification\n",
      "2346   one-drop rule\n",
      "2347   global distributions\n",
      "2348   doctors\n",
      "2349   1970s\n",
      "2350   differences\n",
      "2351   1996\n",
      "2352   2007 review paper\n",
      "2353   least three million people\n",
      "2354   races\n",
      "2355   early 20th century\n",
      "2356   categories\n",
      "2357   more realistic portrayal\n",
      "2358   visually\n",
      "2359   19th century\n",
      "2360   race\n",
      "2361   english language\n",
      "2362   considerable suffering\n",
      "2363   term\n",
      "2364   england\n",
      "2365   cladistics\n",
      "2366   concept\n",
      "2367   , therefore fst values\n",
      "2368   replaced\n",
      "2369   same geographical location\n",
      "2370   2007 review paper\n",
      "2371   homininae\n",
      "2372   scholars\n",
      "2373   rachel caspari\n",
      "2374   scientists\n",
      "2375   came\n",
      "2376   than 1.8 million years ago\n",
      "2377   virtually all physical anthropologists\n",
      "2378   figure\n",
      "2379   stanford university school of medicine\n",
      "2380   correctly\n",
      "2381   attempt\n",
      "2382   era\n",
      "2383   farm\n",
      "2384   temple area\n",
      "2385   evidence\n",
      "2386   social inequality\n",
      "2387   settlements\n",
      "2388   neolithic societies\n",
      "2389   southwestern united states\n",
      "2390   era\n",
      "2391   rhine\n",
      "2392   era\n",
      "2393   elaborate tombs\n",
      "2394   mid-late neolithic\n",
      "2395   elaborate tombs\n",
      "2396   cultural elements\n",
      "2397   2002\n",
      "2398   control\n",
      "2399   first\n",
      "2400   farm\n",
      "2401   2012\n",
      "2402   copper age\n",
      "2403   southeast europe agrarian societies\n",
      "2404   era\n",
      "2405   eurasia\n",
      "2406   beginning\n",
      "2407   neolithic pastoralists\n",
      "2408   bronze age\n",
      "2409   anthropomorphic figurines\n",
      "2410   site\n",
      "2411   middle east\n",
      "2412   turkey\n",
      "2413   early farmers\n",
      "2414   near east\n",
      "2415   control\n",
      "2416   era\n",
      "2417   vashtëmi\n",
      "2418   early farmers\n",
      "2419   9000 bc\n",
      "2420   neolithic people\n",
      "2421   era\n",
      "2422   çatal höyük\n",
      "2423   early japanese societies\n",
      "2424   9000 bc\n",
      "2425   largest prehistoric settlements\n",
      "2426   parts\n",
      "2427   10,700\n",
      "2428   ireland\n",
      "2429   treaties\n",
      "2430   same hierarchical position\n",
      "2431   contracting parties ' full names\n",
      "2432   two-thirds\n",
      "2433   relation\n",
      "2434   sovereignty\n",
      "2435   treaty\n",
      "2436   party\n",
      "2437   citation\n",
      "2438   case\n",
      "2439   congressional-executive agreements\n",
      "2440   office of legal affairs\n",
      "2441   one party\n",
      "2442   600\n",
      "2443   attack\n",
      "2444   material breach\n",
      "2445   treaties\n",
      "2446   consent\n",
      "2447   treaties\n",
      "2448   parties\n",
      "2449   cases\n",
      "2450   development\n",
      "2451   state\n",
      "2452   citation\n",
      "2453   al-waqidi\n",
      "2454   article\n",
      "2455   international law\n",
      "2456   binding greenhouse gas emission limits\n",
      "2457   brazil\n",
      "2458   two\n",
      "2459   treaties\n",
      "2460   long treaty\n",
      "2461   subsequent act\n",
      "2462   compacts\n",
      "2463   state legislation\n",
      "2464   state party's withdrawal\n",
      "2465   treaty\n",
      "2466   modern treaties\n",
      "2467   signatures\n",
      "2468   language\n",
      "2469   entered\n",
      "2470   state\n",
      "2471   political boundaries\n",
      "2472   unaccepting\n",
      "2473   international law\n",
      "2474   treaty\n",
      "2475   congress\n",
      "2476   official legal procedures\n",
      "2477   party\n",
      "2478   state party's withdrawal\n",
      "2479   prior to 1871\n",
      "2480   united nations\n",
      "2481   government\n",
      "2482   mabo\n",
      "2483   un\n",
      "2484   case\n",
      "2485   preamble\n",
      "2486   example\n",
      "2487   brazilian federal constitution\n",
      "2488   verb\n",
      "2489   european colonization\n",
      "2490   treaties\n",
      "2491   citation\n",
      "2492   contracting parties ' full names\n",
      "2493   division\n",
      "2494   obligations\n",
      "2495   supreme court\n",
      "2496   state party's withdrawal\n",
      "2497   late 20th and early 21st century\n",
      "2498   consent\n",
      "2499   treaty\n",
      "2500   division\n",
      "2501   text\n",
      "2502   terminology\n",
      "2503   date\n",
      "2504   citation\n",
      "2505   party's consent\n",
      "2506   treaty\n",
      "2507   multilateral treaties\n",
      "2508   norms\n",
      "2509   three\n",
      "2510   international covenant on civil and political rights\n",
      "2511   federal supreme court\n",
      "2512   brazilian federal constitution\n",
      "2513   state's consent\n",
      "2514   coercion\n",
      "2515   ethiopia\n",
      "2516   practice\n",
      "2517   u.s.\n",
      "2518   case\n",
      "2519   consent\n",
      "2520   withdrawal\n",
      "2521   citation\n",
      "2522   vienna convention\n",
      "2523   treaty\n",
      "2524   interest\n",
      "2525   reservations\n",
      "2526   coercion\n",
      "2527   treaties\n",
      "2528   chapter headings\n",
      "2529   deal\n",
      "2530   long-term, complex legal obligations\n",
      "2531   treaty\n",
      "2532   minor corrections\n",
      "2533   executive agreement\n",
      "2534   article\n",
      "2535   consent\n",
      "2536   treaties\n",
      "2537   45th amendment\n",
      "2538   state's consent\n",
      "2539   treaty\n",
      "2540   normal legislation process\n",
      "2541   relative ease\n",
      "2542   strong presumption\n",
      "2543   treaty breach\n",
      "2544   instance\n",
      "2545   universally accepted prohibitions\n",
      "2546   citation\n",
      "2547   many parties\n",
      "2548   parties\n",
      "2549   consent\n",
      "2550   amend\n",
      "2551   citation\n",
      "2552   subject\n",
      "2553   un\n",
      "2554   union\n",
      "2555   citation\n",
      "2556   treaties\n",
      "2557   end\n",
      "2558   international relations\n",
      "2559   international relations\n",
      "2560   muhammad\n",
      "2561   same reservations\n",
      "2562   nations\n",
      "2563   state\n",
      "2564   treaty\n",
      "2565   particular interpretation\n",
      "2566   treaty\n",
      "2567   eu\n",
      "2568   treaties\n",
      "2569   international law\n",
      "2570   multiple paragraphs\n",
      "2571   state\n",
      "2572   citation\n",
      "2573   contracting parties ' full names\n",
      "2574   citation\n",
      "2575   example\n",
      "2576   terminology\n",
      "2577   example\n",
      "2578   dates\n",
      "2579   state\n",
      "2580   many treaties\n",
      "2581   president\n",
      "2582   gabriel\n",
      "2583   citation\n",
      "2584   case\n",
      "2585   complaint\n",
      "2586   articles\n",
      "2587   many treaties\n",
      "2588   treaties\n",
      "2589   2008\n",
      "2590   dates\n",
      "2591   ruled\n",
      "2592   agreement\n",
      "2593   add\n",
      "2594   articles\n",
      "2595   states\n",
      "2596   reservations\n",
      "2597   supreme court\n",
      "2598   passenger elevator\n",
      "2599   40-50 %\n",
      "2600   1853\n",
      "2601   weight\n",
      "2602   counterweights\n",
      "2603   opens\n",
      "2604   berlin\n",
      "2605   werner von siemens\n",
      "2606   elevator cab\n",
      "2607   germany\n",
      "2608   low-volume hours\n",
      "2609   double deck elevators\n",
      "2610   make\n",
      "2611   citation\n",
      "2612   israeli\n",
      "2613   hydraulic crane\n",
      "2614   four\n",
      "2615   relay controller\n",
      "2616   city transport\n",
      "2617   governor device\n",
      "2618   passenger cabs\n",
      "2619   unique design characteristics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2620   residential\n",
      "2621   end\n",
      "2622   safety record\n",
      "2623   residential\n",
      "2624   300,000\n",
      "2625   passenger elevators\n",
      "2626   less expensive installations\n",
      "2627   1\n",
      "2628   gearless traction machines\n",
      "2629   steam driven devices\n",
      "2630   cab interiors\n",
      "2631   another way\n",
      "2632   dumbwaiters\n",
      "2633   freight elevator\n",
      "2634   first elevator shaft\n",
      "2635   january\n",
      "2636   construction\n",
      "2637   single bulkhead cylinders\n",
      "2638   machine-room-less elevators\n",
      "2639   `` shaft\n",
      "2640   neapolitan architect\n",
      "2641   barrel\n",
      "2642   40-50 %\n",
      "2643   london\n",
      "2644   method\n",
      "2645   environmental concerns\n",
      "2646   belt elevators\n",
      "2647   team's earliest exit\n",
      "2648   1189\n",
      "2649   eight\n",
      "2650   1953\n",
      "2651   citation\n",
      "2652   1872\n",
      "2653   2001 and 2006\n",
      "2654   february 2012\n",
      "2655   1954 fifa world cup\n",
      "2656   1974\n",
      "2657   england\n",
      "2658   2008–09 season\n",
      "2659   england national football team\n",
      "2660   \n",
      "2661   charge\n",
      "2662   england\n",
      "2663   2002\n",
      "2664   england\n",
      "2665   john terry\n",
      "2666   2002\n",
      "2667   england's traditional away colours\n",
      "2668   1923\n",
      "2669   england\n",
      "2670   game\n",
      "2671   setanta sports's\n",
      "2672   away kit\n",
      "2673   roy hodgson\n",
      "2674   1966\n",
      "2675   motif\n",
      "2676   citation\n",
      "2677   pouka\n",
      "2678   2011\n",
      "2679   annual quota\n",
      "2680   gradual sea-level rise\n",
      "2681   july\n",
      "2682   2013 pacific mini games\n",
      "2683   designates\n",
      "2684   18 february\n",
      "2685   trading companies\n",
      "2686   tuvalu\n",
      "2687   vasafua\n",
      "2688   vasafua\n",
      "2689   ano\n",
      "2690   artefacts\n",
      "2691   30%\n",
      "2692   claim\n",
      "2693   tuvalu\n",
      "2694   traditional sport\n",
      "2695   pacific games\n",
      "2696   merchant marine fleet\n",
      "2697   funafuti\n",
      "2698   united nations special rapporteur\n",
      "2699   less than a year\n",
      "2700   teacher-pupil ratio\n",
      "2701   stamps\n",
      "2702   tuvalu\n",
      "2703   tuvalu\n",
      "2704   material culture\n",
      "2705   important creation myth\n",
      "2706   nauru agreement\n",
      "2707   funafuti\n",
      "2708   princess margaret hospital\n",
      "2709   tuvalu media department\n",
      "2710   2011 report\n",
      "2711   65 %\n",
      "2712   keith s. chambers\n",
      "2713   another caste\n",
      "2714   1861\n",
      "2715   26 square kilometres\n",
      "2716   william rainbow\n",
      "2717   janet nicoll\n",
      "2718   captain davis\n",
      "2719   house\n",
      "2720   may 1819\n",
      "2721   consequence\n",
      "2722   tuvalu\n",
      "2723   trading companies\n",
      "2724   low elevation\n",
      "2725   schuyler de peyster\n",
      "2726   islands\n",
      "2727   other members\n",
      "2728   traditional foods\n",
      "2729   sixth form students\n",
      "2730   year\n",
      "2731   davis\n",
      "2732   5 september\n",
      "2733   charles hedley\n",
      "2734   island\n",
      "2735   tuvalu media department\n",
      "2736   established\n",
      "2737   rulings\n",
      "2738   water\n",
      "2739   english\n",
      "2740   cyclone bebe\n",
      "2741   major international priority\n",
      "2742   coral bleaching\n",
      "2743   $ $ 2.2 million\n",
      "2744   tuvalu participates\n",
      "2745   1900\n",
      "2746   islands\n",
      "2747   two\n",
      "2748   170\n",
      "2749   name\n",
      "2750   islets\n",
      "2751   funafuti\n",
      "2752   funafuti\n",
      "2753   three\n",
      "2754   traditional sport\n",
      "2755   2002\n",
      "2756   installation\n",
      "2757   traditional foods\n",
      "2758   new am radio transmitter\n",
      "2759   three\n",
      "2760   1892\n",
      "2761   great coconut plantation\n",
      "2762   first decade\n",
      "2763   increase\n",
      "2764   tuvalu media department\n",
      "2765   resulting borrow pits\n",
      "2766   nine or ten\n",
      "2767   sixth form students\n",
      "2768   world bank\n",
      "2769   highest elevations\n",
      "2770   english\n",
      "2771   two\n",
      "2772   tuvalu\n",
      "2773   nivaga iii\n",
      "2774   tuvalu overseas seamen's union ( tosu\n",
      "2775   tuvalu\n",
      "2776   national water resources policy\n",
      "2777   2013\n",
      "2778   1819\n",
      "2779   tuvaluan order\n",
      "2780   eight island courts and lands courts\n",
      "2781   tuvalu\n",
      "2782   remittances\n",
      "2783   tropical depression\n",
      "2784   english\n",
      "2785   english\n",
      "2786   funafuti\n",
      "2787   tuvalu government\n",
      "2788   tuvalu\n",
      "2789   charted\n",
      "2790   english\n",
      "2791   tuvalu media department\n",
      "2792   10 years\n",
      "2793   eight per cent\n",
      "2794   japan international cooperation agency\n",
      "2795   program\n",
      "2796   presidents\n",
      "2797   european union\n",
      "2798   pacific war funafuti\n",
      "2799   trader\n",
      "2800   nine islands\n",
      "2801   funafuti\n",
      "2802   tuvalu advocates ratification\n",
      "2803   major international priority\n",
      "2804   international monetary fund 2010 report\n",
      "2805   independence\n",
      "2806   part\n",
      "2807   another important building\n",
      "2808   manu\n",
      "2809   tuvalu ship registry\n",
      "2810   10,640\n",
      "2811   july 2012\n",
      "2812   palagi traders\n",
      "2813   hearing\n",
      "2814   new imac\n",
      "2815   68020-powered macintosh lc\n",
      "2816   macintosh\n",
      "2817   2,600\n",
      "2818   may 1998\n",
      "2819   first portable computer\n",
      "2820   then-new imac g5\n",
      "2821   order\n",
      "2822   created\n",
      "2823   macintosh project\n",
      "2824   sold\n",
      "2825   four years\n",
      "2826   current mac product family\n",
      "2827   39 advertising pages\n",
      "2828   sales\n",
      "2829   macintosh\n",
      "2830   mac os x\n",
      "2831   apple's market share\n",
      "2832   pcs\n",
      "2833   mac's market share\n",
      "2834   retrofits\n",
      "2835   use\n",
      "2836   air\n",
      "2837   apple\n",
      "2838   intel\n",
      "2839   original\n",
      "2840   same year\n",
      "2841   response\n",
      "2842   matters\n",
      "2843   macintosh\n",
      "2844   may\n",
      "2845   well\n",
      "2846   john sculley\n",
      "2847   one\n",
      "2848   may\n",
      "2849   steve jobs\n",
      "2850   1998\n",
      "2851   october\n",
      "2852   macintosh\n",
      "2853   macintosh ii\n",
      "2854   $ $ 2.5 million\n",
      "2855   steve jobs\n",
      "2856   may 1990\n",
      "2857   new imac\n",
      "2858   us $ 1.5 million\n",
      "2859   39 advertising pages\n",
      "2860   estimated 100,000\n",
      "2861   jef raskin\n",
      "2862   2000\n",
      "2863   john dvorak\n",
      "2864   1997\n",
      "2865   ten\n",
      "2866   macintosh platform\n",
      "2867   august\n",
      "2868   1989\n",
      "2869   second generation\n",
      "2870   change\n",
      "2871   final macintosh design\n",
      "2872   mice\n",
      "2873   apple\n",
      "2874   apple\n",
      "2875   early 2001\n",
      "2876   primary authors\n",
      "2877   installed base\n",
      "2878   imac\n",
      "2879   finally\n",
      "2880   macintosh's minimal memory\n",
      "2881   clarisworks\n",
      "2882   2000\n",
      "2883   complete office suite\n",
      "2884   number five spot\n",
      "2885   third-party platform virtualization software\n",
      "2886   personal computer manufacturers\n",
      "2887   steve jobs\n",
      "2888   quarterly\n",
      "2889   air\n",
      "2890   open firmware\n",
      "2891   new motorola 68030 processor\n",
      "2892   models\n",
      "2893   contrast\n",
      "2894   steve jobs\n",
      "2895   bill atkinson\n",
      "2896   macintosh classic\n",
      "2897   macintosh's market share\n",
      "2898   discontinued power macintosh g3 , to slot between the imac g3 and the power mac g4 .\n",
      "2899   hewlett packard\n",
      "2900   suggestions\n",
      "2901   october 2009\n",
      "2902   several ultrabooks\n",
      "2903   mavericks\n",
      "2904   chief designer\n",
      "2905   allan loren\n",
      "2906   updated motorola cpus\n",
      "2907   1985\n",
      "2908   may 1990\n",
      "2909   october\n",
      "2910   open firmware\n",
      "2911   apple\n",
      "2912   3.36 million\n",
      "2913   macintosh se\n",
      "2914   macbook air\n",
      "2915   2013\n",
      "2916   mid-2011\n",
      "2917   darwin\n",
      "2918   macintosh\n",
      "2919   march\n",
      "2920   c++\n",
      "2921   current mac models\n",
      "2922   cube\n",
      "2923   apple's actions\n",
      "2924   one megabyte\n",
      "2925   14 %\n",
      "2926   c++\n",
      "2927   apple\n",
      "2928   performance advantage\n",
      "2929   20\n",
      "2930   2006\n",
      "2931   time\n",
      "2932   windows 95 operating system\n",
      "2933   windows\n",
      "2934   apple\n",
      "2935   smith's design\n",
      "2936   new imac\n",
      "2937   \n",
      "2938   part\n",
      "2939   third place spot\n",
      "2940   macintosh\n",
      "2941   imac , macbook pro , macbook air , and mac mini lines\n",
      "2942   early 2011\n",
      "2943   models\n",
      "2944   materials\n",
      "2945   component shortage\n",
      "2946   seven years\n",
      "2947   recent years\n",
      "2948   two\n",
      "2949   2006\n",
      "2950   macintosh\n",
      "2951   removal\n",
      "2952   1993 powerbook 165c\n",
      "2953   macintosh iici\n",
      "2954   mid-2011\n",
      "2955   october\n",
      "2956   compaq\n",
      "2957   jean-louis gassée\n",
      "2958   lisa and macintosh user interfaces\n",
      "2959   regis mckenna\n",
      "2960   launch\n",
      "2961   performance advantage\n",
      "2962   update\n",
      "2963   mac os x\n",
      "2964   personal computer manufacturers\n",
      "2965   ten\n",
      "2966   macintosh\n",
      "2967   1993\n",
      "2968   crt\n",
      "2969   macintosh's market share\n",
      "2970   one\n",
      "2971   glass\n",
      "2972   9,900\n",
      "2973   march 2006\n",
      "2974   floppy disk drive\n",
      "2975   apple\n",
      "2976   56 percent\n",
      "2977   macintosh programmer's workshop\n",
      "2978   1994\n",
      "2979   floppy disk drive\n",
      "2980   april 1984\n",
      "2981   apple\n",
      "2982   macs\n",
      "2983   ten\n",
      "2984   every macintosh\n",
      "2985   3,195\n",
      "2986   mac\n",
      "2987   launch\n",
      "2988   2011\n",
      "2989   software business\n",
      "2990   2.06 percent\n",
      "2991   virbhadra singh\n",
      "2992   state-owned television broadcaster\n",
      "2993   governor\n",
      "2994   himachal\n",
      "2995   sikhs\n",
      "2996   kangra miniature paintings\n",
      "2997   state\n",
      "2998   himachal\n",
      "2999   state\n",
      "3000   himachal pradesh\n",
      "3001   economy\n",
      "3002   10th century\n",
      "3003   hill stations\n",
      "3004   public radio station\n",
      "3005   1 november 1956\n",
      "3006   himachal pradesh\n",
      "3007   handicrafts\n",
      "3008   virbhadra singh\n",
      "3009   21st\n",
      "3010   anglo-gorkha war\n",
      "3011   superintendent\n",
      "3012   pit-loom\n",
      "3013   virbhadra singh\n",
      "3014   buddhism and sikhism\n",
      "3015   himachal pradesh\n",
      "3016   wool\n",
      "3017   year 1768\n",
      "3018   hima\n",
      "3019   hydro electric resources\n",
      "3020   dial-up access\n",
      "3021   mahmud ghaznavi\n",
      "3022   citation\n",
      "3023   citation\n",
      "3024   technological advancements\n",
      "3025   30 december 2007\n",
      "3026   first\n",
      "3027   nepal\n",
      "3028   h.p.\n",
      "3029   private fm stations\n",
      "3030   18 december 1970\n",
      "3031   state\n",
      "3032   1.16%\n",
      "3033   radio\n",
      "3034   2006\n",
      "3035   handicrafts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3036   1948\n",
      "3037   muslims\n",
      "3038   first five-year plan\n",
      "3039   extreme variation\n",
      "3040   others\n",
      "3041   military settlement\n",
      "3042   roman women\n",
      "3043   imperial era\n",
      "3044   human sacrifice\n",
      "3045   security\n",
      "3046   edict\n",
      "3047   public festivals\n",
      "3048   camp\n",
      "3049   constantine\n",
      "3050   roman women\n",
      "3051   second edict\n",
      "3052   famous tirade\n",
      "3053   human sacrifice\n",
      "3054   strong connections\n",
      "3055   ordinary romans\n",
      "3056   vergil\n",
      "3057   valerian's first religious edict\n",
      "3058   di immortales\n",
      "3059   roman\n",
      "3060   roman camps\n",
      "3061   ruins\n",
      "3062   abolition\n",
      "3063   excessive devotion\n",
      "3064   rome's hegemony\n",
      "3065   end\n",
      "3066   product\n",
      "3067   customary offers\n",
      "3068   product\n",
      "3069   rome's hegemony\n",
      "3070   relationship\n",
      "3071   little or no civil authority\n",
      "3072   edict\n",
      "3073   wake\n",
      "3074   most important camp-offering\n",
      "3075   several days\n",
      "3076   women\n",
      "3077   dictator\n",
      "3078   public festivals\n",
      "3079   edict\n",
      "3080   opportunities\n",
      "3081   solution\n",
      "3082   cult\n",
      "3083   sporadic and sometimes brutal attempts\n",
      "3084   punic crisis\n",
      "3085   human sacrifice\n",
      "3086   roman oaths\n",
      "3087   least\n",
      "3088   office\n",
      "3089   jupiter latiaris\n",
      "3090   religious dimensions\n",
      "3091   augustus\n",
      "3092   roman calendar\n",
      "3093   second edict\n",
      "3094   brought\n",
      "3095   lares and penates\n",
      "3096   first punic war\n",
      "3097   non-official but lawful cults\n",
      "3098   rites\n",
      "3099   act\n",
      "3100   same token\n",
      "3101   superstitio\n",
      "3102   jupiter feretrius\n",
      "3103   others\n",
      "3104   vestals\n",
      "3105   augustan settlement\n",
      "3106   february\n",
      "3107   hitherto unlikely source\n",
      "3108   haruspices\n",
      "3109   valerian's first religious edict\n",
      "3110   gaul\n",
      "3111   original meaning\n",
      "3112   everyday world\n",
      "3113   early severan era\n",
      "3114   roman bellona\n",
      "3115   heretics\n",
      "3116   last emperor\n",
      "3117   roman calendar\n",
      "3118   twelve tables\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-918c4a7ffa64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# sentence as a document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mraw_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m-> 1241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \"\"\"\n\u001b[0;32m-> 1291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \"\"\"\n\u001b[0;32m-> 1291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mre_boundary_realignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csvFile = open(\"high.csv\", \"w\")\n",
    "writer = csv.writer(csvFile)\n",
    "header = ['id','answer']\n",
    "writer.writerow(header)\n",
    "\n",
    "\n",
    "case_count = 0\n",
    "# test = [test[17]]\n",
    "for test_case in test:\n",
    "    question = test_case['question']\n",
    "    docid = test_case['docid']\n",
    "\n",
    "    # Convert doc into one string, then tokenize sentences\n",
    "    corpus = ''\n",
    "    for para in documents[docid]['text']:\n",
    "        corpus += para + ' '\n",
    "\n",
    "    # sentence as a document\n",
    "    raw_docs = nltk.sent_tokenize(corpus)\n",
    "    \n",
    "\n",
    "    # TFIDF\n",
    "    doc_term_freqs = {}\n",
    "    for (id, raw_doc) in enumerate(raw_docs):\n",
    "        term_freqs = extract_term_freqs(raw_doc)\n",
    "        doc_term_freqs[id] = term_freqs\n",
    "    M = len(doc_term_freqs)\n",
    "\n",
    "    doc_freqs = compute_doc_freqs(doc_term_freqs)\n",
    "\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    for docid, term_freqs in doc_term_freqs.items():\n",
    "        N = sum(term_freqs.values())\n",
    "        length = 0\n",
    "\n",
    "        # find tf*idf values and accumulate sum of squares\n",
    "        tfidf_values = []\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(doc_freqs[term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # inversion of the indexing, term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([docid, tfidf / length])\n",
    "\n",
    "    for term, docids in vsm_inverted_index.items():\n",
    "        docids.sort()\n",
    "\n",
    "    terms = extract_term_freqs(question) \n",
    "    results = query_vsm(terms, vsm_inverted_index)\n",
    "    \n",
    "#     tokenized_sentence = []\n",
    "#     for each_sentence in raw_docs:\n",
    "#         sentence_as_words = nltk.word_tokenize(each_sentence)\n",
    "#         tokenized_sentence.append(sentence_as_words)\n",
    "        \n",
    "#     bm25Model = bm25.BM25(tokenized_sentence)\n",
    "#     average_idf = sum(map(lambda k: float(bm25Model.idf[k]), bm25Model.idf.keys())) / len(bm25Model.idf.keys())\n",
    "    \n",
    "#     query = []\n",
    "#     for word in nltk.word_tokenize(question):\n",
    "#         query.append(word)\n",
    "        \n",
    "#     scores = bm25Model.get_scores(query,average_idf)\n",
    "#     bm25_dic = Counter()\n",
    "#     sentence_id = 0\n",
    "#     for each_score in scores:\n",
    "#         bm25_dic[sentence_id] = each_score\n",
    "#         sentence_id += 1\n",
    "#     results = bm25_dic.most_common(4)\n",
    "\n",
    "\n",
    "    # Step 2\n",
    "    # Analyse question type\n",
    "    qword = get_qword(question)\n",
    "\n",
    "    # the word after question word, such as 'what value', 'which gender'\n",
    "    next_token = ''\n",
    "\n",
    "    qtype = ''\n",
    "\n",
    "    # dependency parsing\n",
    "    dep = ''\n",
    "\n",
    "    # head word\n",
    "    head = ''\n",
    "\n",
    "    # head dependency\n",
    "    head_dep = ''\n",
    "\n",
    "    # subject, root, object\n",
    "    nsubj = ''\n",
    "    ROOT = ''\n",
    "    dobj = ''\n",
    "\n",
    "    # yes or no questions have two options\n",
    "    closed_q_choices = ('', '')\n",
    "\n",
    "    doc = nlp(question)\n",
    "\n",
    "    tokens = nltk.word_tokenize(question.lower())\n",
    "\n",
    "    # get next word\n",
    "    if qword in tokens:\n",
    "        if tokens.index(qword) < len(tokens) - 1:\n",
    "            next_token = tokens[tokens.index(qword) + 1]\n",
    "\n",
    "    # get structure of sentence\n",
    "    for token in doc:\n",
    "        if 'nsubj' in token.dep_:\n",
    "            nsubj = lemmatize(strip_punctuation(token.text))\n",
    "        if token.dep_ == 'ROOT':\n",
    "            ROOT = lemmatize(strip_punctuation(token.text))\n",
    "        if 'dobj' in token.dep_:\n",
    "            dobj = lemmatize(strip_punctuation(token.text))\n",
    "\n",
    "    # for noun (phrase) questions, get answer dependency\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if qword in chunk.text:\n",
    "            dep = chunk.root.dep_\n",
    "            head = lemmatize(strip_punctuation(chunk.root.head.text))\n",
    "            head_dep = chunk.root.head.dep_\n",
    "\n",
    "    # determine answer type\n",
    "    if 'stand for' in question or 'abbreviat' in question:\n",
    "        qtype = 'abrv'\n",
    "\n",
    "    elif qword in ['who',\"who's\",'whom','whose']:\n",
    "        qtype = 'who'\n",
    "\n",
    "    elif qword == 'when':\n",
    "        qtype = 'when'\n",
    "\n",
    "    elif qword in ['where',\"where's\"]:\n",
    "        qtype = 'where'\n",
    "\n",
    "    elif qword in ['how',\"how's\"]:\n",
    "        if next_token == 'much':\n",
    "            qtype = 'MONEY'\n",
    "        elif next_token == 'many':\n",
    "            qtype = 'CARDINAL'\n",
    "        elif next_token == 'long':\n",
    "            qtype = 'DATE'\n",
    "        elif next_token in ['far','big','wide','deep','tall','high','fast','heavy']:\n",
    "            qtype = 'QUANTITY'\n",
    "        elif next_token in ['old','young']:\n",
    "            qtype = 'DATE'\n",
    "        elif next_token in ['does','did','do','have','has','had','should',\n",
    "                              'can','could','will','would','must']:\n",
    "            if dobj != '':\n",
    "                qtype = 'adj'\n",
    "            else:\n",
    "                qtype = 'verb'\n",
    "\n",
    "    elif qword in ['what', \"what's\", 'which']:\n",
    "        if 'year'in tokens or \\\n",
    "                'day' in tokens or \\\n",
    "                'month' in tokens or \\\n",
    "                'era' in tokens or \\\n",
    "                'age' in tokens or \\\n",
    "                'century' in tokens or \\\n",
    "                'week' in tokens or \\\n",
    "                'period' in tokens or \\\n",
    "                'dynasty' in tokens:\n",
    "            qtype = 'DATE'\n",
    "\n",
    "        elif 'company' in tokens or \\\n",
    "                'organization' in tokens or \\\n",
    "                'organisation' in tokens or \\\n",
    "                'corporation' in tokens or \\\n",
    "                'institution' in tokens or \\\n",
    "                'university' in tokens or \\\n",
    "                'corporation' in tokens or \\\n",
    "                'association' in tokens or \\\n",
    "                'union' in tokens or \\\n",
    "                'agency' in tokens:\n",
    "            qtype = 'ORG'\n",
    "\n",
    "        elif 'city' in tokens or \\\n",
    "                'country' in tokens or \\\n",
    "                'state' in tokens or \\\n",
    "                'province' in tokens or \\\n",
    "                'county' in tokens:\n",
    "            qtype = 'GPE'\n",
    "\n",
    "        elif 'place' in tokens or \\\n",
    "                'river' in tokens or \\\n",
    "                'mountain' in tokens or \\\n",
    "                'ocean' in tokens or \\\n",
    "                'region' in tokens or \\\n",
    "                'area' in tokens or \\\n",
    "                'sea' in tokens or \\\n",
    "                'lake' in tokens or \\\n",
    "                'continent' in tokens or \\\n",
    "                'location' in tokens or \\\n",
    "                'forest' in tokens or \\\n",
    "                'jungle' in tokens:\n",
    "            qtype = 'LOC'\n",
    "\n",
    "        elif 'nationality' in tokens:\n",
    "            qtype = 'NORP'\n",
    "\n",
    "        elif 'building' in tokens or \\\n",
    "            'airport' in tokens or \\\n",
    "            'highway' in tokens or \\\n",
    "            'bridge' in tokens or \\\n",
    "            'harbour' in tokens or \\\n",
    "            'harbor' in tokens or \\\n",
    "            'port' in tokens or \\\n",
    "            'dam' in tokens:\n",
    "            qtype = 'FACILITY'\n",
    "\n",
    "        elif 'hurricane' in tokens or \\\n",
    "            'battle' in tokens or \\\n",
    "            'war' in tokens:\n",
    "            qtype = 'EVENT'\n",
    "\n",
    "        elif 'book' in tokens or \\\n",
    "            'novel' in tokens or \\\n",
    "            'song' in tokens or \\\n",
    "            'music' in tokens or \\\n",
    "            'painting' in tokens:\n",
    "            qtype = 'WORK_OF_ART'\n",
    "\n",
    "        elif 'language' in tokens or \\\n",
    "                'speak' in tokens:\n",
    "            qtype = 'LANGUAGE'\n",
    "\n",
    "        elif 'percentage' in tokens or 'percent' in tokens:\n",
    "            qtype = 'PERCENT'\n",
    "\n",
    "        elif 'value' in tokens or \\\n",
    "                'distance' in tokens or \\\n",
    "                'size' in tokens or \\\n",
    "                'length' in tokens or \\\n",
    "                'depth' in tokens or \\\n",
    "                'height' in tokens or \\\n",
    "                'density' in tokens or \\\n",
    "                'speed' in tokens or \\\n",
    "                'weight' in tokens or \\\n",
    "                'area' in tokens or \\\n",
    "                'temperature' in tokens or \\\n",
    "                'volume' in tokens:\n",
    "            qtype = 'QUANTITY'\n",
    "\n",
    "        elif 'number' in tokens:\n",
    "            qtype = 'CARDINAL'\n",
    "\n",
    "        elif 'price' in tokens:\n",
    "            qtype = 'MONEY'\n",
    "\n",
    "        elif 'name' in tokens:\n",
    "            qtype = 'NE'\n",
    "\n",
    "        else:\n",
    "            # what...do type question\n",
    "            tokens.remove(next_token)\n",
    "            if 'do' in tokens:\n",
    "                qtype = 'verb'\n",
    "            else:\n",
    "                qtype = 'noun'\n",
    "\n",
    "    elif qword == 'why':\n",
    "        qtype = 'why'\n",
    "\n",
    "    elif qword in CLOSED_QUESTION_WORDS:\n",
    "        qtype = 'closed'\n",
    "\n",
    "        # answer is one of the 'or' options in the question\n",
    "        if 'or' in tokens:\n",
    "            index = tokens.index('or')\n",
    "            prev1 = tokens[index - 1]\n",
    "            next1 = tokens[index + 1]\n",
    "            tag_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "            tag = tag_tokens[index - 1][1]\n",
    "\n",
    "            # if answer is a noun\n",
    "            if tag in ['NN', 'NNP', 'NNS', 'NNPS']:\n",
    "                for chunk in doc.noun_chunks:\n",
    "                    if prev1 in chunk.text:\n",
    "                        first = chunk.text\n",
    "                    if next1 in chunk.text:\n",
    "                        second = chunk.text\n",
    "                closed_q_choices = (first, second)\n",
    "            else:\n",
    "                closed_q_choices = (prev1, next1)\n",
    "        else:\n",
    "            qtype = 'others'\n",
    "\n",
    "    # re-rank the 20 sentences\n",
    "    scores = {}\n",
    "    for id, _ in results:\n",
    "        sent = raw_docs[id]\n",
    "        doc = nlp(sent)\n",
    "\n",
    "        score = get_overlap(sent, question)\n",
    "\n",
    "        if qtype == 'who':\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PERSON':\n",
    "                    score += 1\n",
    "\n",
    "        elif qtype == 'when':\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'TIME' or ent.label_ == \"DATE\":\n",
    "                    score += 1\n",
    "\n",
    "        elif qtype == 'where':\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'GPE' or ent.label_ == \"LOC\":\n",
    "                    score += 1\n",
    "\n",
    "        elif qtype in ['LANGUAGE','WORK_OF_ART','EVENT','NORP','FACILITY',\n",
    "                       'GPE','DATE','TIME','PERCENT','QUANTITY','CARDINAL',\n",
    "                     'MONEY','PERSON','ORG','LOC']:\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == qtype:\n",
    "                    score += 1\n",
    "                    \n",
    "        elif qtype == 'NE':\n",
    "            for ent in doc.ents:\n",
    "                    score += 1\n",
    "\n",
    "        elif qtype == 'adj':\n",
    "            for token in doc:\n",
    "                if 'advmod' in token.dep_ or 'acomp' in token.dep_:\n",
    "                    score += 1\n",
    "\n",
    "        elif qtype == 'verb':\n",
    "            for token in doc:\n",
    "                if token.dep_ == 'ROOT':\n",
    "                    score += 1\n",
    "\n",
    "        elif qtype == 'closed':\n",
    "            first = closed_q_choices[0]\n",
    "            second = closed_q_choices[1]\n",
    "\n",
    "            score += (first in sent) + (second in sent)\n",
    "\n",
    "        elif qtype == 'why':\n",
    "            if 'reason' in sent or 'because' in sent or 'due to' in sent or 'since' in sent or 'for' in sent:\n",
    "                score += 1\n",
    "\n",
    "        scores[id] = score\n",
    "\n",
    "    rank = {}\n",
    "    for id, sim in results:\n",
    "        max_score = scores[max(scores, key=scores.get)]\n",
    "        if max_score != 0:\n",
    "            rank[id] = sim * 0.5 + (scores[id] / max_score * 0.5)\n",
    "        else:\n",
    "            rank[id] = sim\n",
    "    \n",
    "    # sentence with highest rank\n",
    "    index = max(rank, key=rank.get)\n",
    "    sent = raw_docs[index]\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # find sentence structure\n",
    "    sent_nsubj = ''\n",
    "    sent_ROOT = ''\n",
    "    sent_dobj = ''\n",
    "    for token in doc:\n",
    "        if 'nsubj' in token.dep_:\n",
    "            sent_nsubj = lemmatize(strip_punctuation(token.text))\n",
    "        if token.dep_ == 'ROOT':\n",
    "            sent_ROOT = lemmatize(strip_punctuation(token.text))\n",
    "        if 'dobj' in token.dep_:\n",
    "            sent_dobj = lemmatize(strip_punctuation(token.text))\n",
    "            \n",
    "    # find answer with highest score\n",
    "    max_score = -1\n",
    "    answer = ''\n",
    "    \n",
    "    if qtype == 'who':\n",
    "        for np in doc.noun_chunks:\n",
    "            score = 0\n",
    "            \n",
    "            if np in doc.ents:\n",
    "                for ent in doc.ents:\n",
    "                    if np.text in ent.text and ent.label_ == 'PERSON':\n",
    "                            score += 3\n",
    "\n",
    "            # find NP dependency\n",
    "            np_dep = np.root.dep_\n",
    "            np_head = lemmatize(strip_punctuation(np.root.head.text))\n",
    "            np_head_dep = np.root.head.dep_\n",
    "\n",
    "            if np_dep == dep:\n",
    "                score += 1\n",
    "            if np_head == head:\n",
    "                score += 1\n",
    "            if np_head_dep == head_dep:\n",
    "                score += 1\n",
    "\n",
    "            if np.text not in question:\n",
    "                score += 1\n",
    "\n",
    "            if strip_punctuation(np.text).strip().lower() not in stop:\n",
    "                score += 1\n",
    "\n",
    "            if np.text.lower() == 'it':\n",
    "                score = -1\n",
    "                \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                answer = np.text\n",
    "\n",
    "    elif qtype == 'when':\n",
    "        for ent in doc.ents:\n",
    "            score = 0\n",
    "            \n",
    "            if ent.label_ == 'TIME' or ent.label_ == \"DATE\":\n",
    "                score += 3\n",
    "                \n",
    "            if ent.text not in question:\n",
    "                score += 1\n",
    "            \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                answer = ent.text\n",
    "\n",
    "    elif qtype == 'where':\n",
    "        for ent in doc.ents:\n",
    "            score = 0\n",
    "            \n",
    "            if ent.label_ == 'GPE' or ent.label_ == \"LOC\":\n",
    "                score += 3\n",
    "\n",
    "            if ent.text not in question:\n",
    "                score += 1\n",
    "                \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                answer = ent.text\n",
    "            \n",
    "    elif qtype in ['LANGUAGE', 'WORK_OF_ART', 'EVENT', 'NORP', 'FACILITY',\n",
    "                   'GPE', 'DATE', 'TIME', 'PERCENT', 'QUANTITY', 'CARDINAL',\n",
    "                   'MONEY', 'PERSON', 'ORG', 'LOC']:\n",
    "        for ent in doc.ents:\n",
    "            score = 0\n",
    "            \n",
    "            if ent.label_ == qtype:\n",
    "                score += 3\n",
    "            \n",
    "            if ent.text not in question:\n",
    "                score += 1\n",
    "                \n",
    "            if qtype in ['LOC','GPE'] and ent.root.tag_ not in ['NN','NNP','NNS','NNPS']:\n",
    "                score -= 2\n",
    "                \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                answer = ent.text\n",
    "                \n",
    "                if qtype in ['MONEY']:\n",
    "                    for token in doc:\n",
    "                        if token.text == '$':\n",
    "                            answer = '$ ' + answer\n",
    "                            \n",
    "                if qtype in ['PERCENT']:\n",
    "                    if 'percent' in answer:\n",
    "                        answer = answer[:answer.index('percent')-1]\n",
    "                            \n",
    "                if qtype in ['PERCENT','QUANTITY','CARDINAL','MONEY']:\n",
    "                    tokens = nltk.word_tokenize(answer)\n",
    "                    i = 0\n",
    "                    answer = ''\n",
    "                    while i < len(tokens):\n",
    "                        if tokens[i].lower() in ['well','about','around','approximately', 'some']:\n",
    "                            del tokens[i]\n",
    "                        else:\n",
    "                            if i+1 < len(tokens) and tokens[i+1] == \"'s\":\n",
    "                                answer += tokens[i]\n",
    "                            else:\n",
    "                                answer += tokens[i] + ' '\n",
    "                            i += 1\n",
    "                    answer = answer.strip()\n",
    "                    \n",
    "    elif qtype == 'NE':\n",
    "        for ent in doc.ents:\n",
    "            score = 3\n",
    "            \n",
    "            if ent.text not in question:\n",
    "                score += 1\n",
    "                \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                answer = ent.text\n",
    "\n",
    "    elif qtype == 'abrv':\n",
    "        abrv = ''\n",
    "        qdoc = nlp(question)\n",
    "        for token in qdoc:\n",
    "            text = token.text\n",
    "            if len(text) >= 2 and text.isupper() and text.isalpha():\n",
    "                abrv = text.lower()\n",
    "\n",
    "        if abrv == '' and 'stand for' in question:\n",
    "            tokens = question.lower().split(' ')\n",
    "            abrv = tokens[tokens.index('stand')-1]\n",
    "\n",
    "        if abrv != '':\n",
    "            tokens = nltk.word_tokenize(sent)\n",
    "            for (i, token) in enumerate(tokens):\n",
    "                if token[0].isupper():\n",
    "                    k = 1\n",
    "                    phrase = token.lower()\n",
    "                    initials = phrase[0]\n",
    "\n",
    "                    while i+k < len(tokens) and tokens[i+k][0].isupper():\n",
    "                        phrase = phrase + ' ' + tokens[i+k].lower()\n",
    "                        initials += tokens[i+k][0].lower()\n",
    "                        k += 1\n",
    "\n",
    "                    phrase = phrase.strip()\n",
    "                    if initials == abrv:\n",
    "                        answer = phrase\n",
    "\n",
    "        else:\n",
    "            tokens = nltk.word_tokenize(question)\n",
    "            for (i, token) in enumerate(tokens):\n",
    "                if token[0].isupper():\n",
    "                    k = 1\n",
    "                    initials = token[0].lower()\n",
    "\n",
    "                    while i + k < len(tokens) and tokens[i + k][0].isupper():\n",
    "                        initials += tokens[i + k][0].lower()\n",
    "                        k += 1\n",
    "\n",
    "                    if len(initials) >= 2:\n",
    "                        answer = initials\n",
    "\n",
    "    elif qtype == 'adj':\n",
    "        for token in doc:\n",
    "            score = 0\n",
    "            \n",
    "            if 'advmod' in token.dep_ or 'acomp' in token.dep_:\n",
    "                score += 3\n",
    "\n",
    "            token_dep = token.dep_\n",
    "            token_head = lemmatize(strip_punctuation(token.head.text))\n",
    "            token_head_dep = token.head.dep_\n",
    "\n",
    "            if token_dep == dep:\n",
    "                score += 1\n",
    "            if token_head == head:\n",
    "                score += 1\n",
    "            if token_head_dep == head_dep:\n",
    "                score += 1\n",
    "\n",
    "            if token.text not in question:\n",
    "                score += 1\n",
    "\n",
    "            if strip_punctuation(token.text).strip().lower() not in stop:\n",
    "                score += 1\n",
    "\n",
    "            if token.text.lower() == 'it':\n",
    "                score = -1\n",
    "\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                answer = token.text\n",
    "\n",
    "    elif qtype == 'verb':\n",
    "        for token in doc:\n",
    "            score = 0\n",
    "\n",
    "            if token.dep_ == 'ROOT':\n",
    "                score += 1\n",
    "\n",
    "            if lemmatize(strip_punctuation(token.text)) not in \\\n",
    "                    [lemmatize(strip_punctuation(s)) for s in nltk.word_tokenize(question)]:\n",
    "                score += 1\n",
    "\n",
    "            if strip_punctuation(token.text).strip().lower() not in stop:\n",
    "                score += 1\n",
    "\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                answer = token.text\n",
    "\n",
    "    elif qtype == 'closed':\n",
    "        first = closed_q_choices[0]\n",
    "        second = closed_q_choices[1]\n",
    "\n",
    "        # whether each option appears (and is negates)\n",
    "        appear1 = False\n",
    "        appear2 = False\n",
    "        negate1 = False\n",
    "        negate2 = False\n",
    "        neg_count = 0\n",
    "        tokens = nltk.word_tokenize(raw_docs[id])\n",
    "\n",
    "        for (index, token) in enumerate(tokens):\n",
    "            if token == 'not' or \"n't\" in token:\n",
    "                neg_count += 1\n",
    "\n",
    "                if index+1 < len(tokens):\n",
    "                    if tokens[index+1] == first:\n",
    "                        negate1 = True\n",
    "                    if tokens[index+1] == second:\n",
    "                        negate2 = True\n",
    "\n",
    "            if token == first:\n",
    "                appear1 = True\n",
    "            if token == second:\n",
    "                appear2 = True\n",
    "\n",
    "        possible_answer = ''\n",
    "        if appear1 and not appear2:\n",
    "            if neg_count % 2 == 1:\n",
    "                possible_answer = second\n",
    "            else:\n",
    "                possible_answer = first\n",
    "\n",
    "        elif appear2 and not appear1:\n",
    "            if neg_count % 2 == 0:\n",
    "                possible_answer = second\n",
    "            else:\n",
    "                possible_answer = first\n",
    "\n",
    "        elif appear1 and appear2:\n",
    "            if negate1 and not negate2:\n",
    "                possible_answer = second\n",
    "            elif negate2 and not negate1:\n",
    "                possible_answer = first\n",
    "            else:\n",
    "                possible_answer = second\n",
    "\n",
    "        if possible_answer != '':\n",
    "            score += 5\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                answer = possible_answer\n",
    "\n",
    "    elif qtype == 'why':\n",
    "\n",
    "        possible_answer = ''\n",
    "        score = 0\n",
    "\n",
    "        if 'reason' in sent or 'because' in sent or 'due to' in sent or 'since' in sent or 'for' in sent:\n",
    "\n",
    "            if 'because of' in sent:\n",
    "                score += 3\n",
    "                index = sent.index('because of')\n",
    "                substr = sent[index+11:]\n",
    "                span = nlp(substr)\n",
    "                for chunk in span.noun_chunks:\n",
    "                    possible_answer = chunk.text\n",
    "                    break\n",
    "\n",
    "            elif 'because' in sent:\n",
    "                score += 3\n",
    "                index = sent.index('because')\n",
    "                substr = sent[index + 8:]\n",
    "                possible_answer = substr\n",
    "\n",
    "            elif 'due to' in sent:\n",
    "                score += 3\n",
    "                index = sent.index('due to')\n",
    "                substr = sent[index+7:]\n",
    "                span = nlp(substr)\n",
    "                for chunk in span.noun_chunks:\n",
    "                    possible_answer = chunk.text\n",
    "                    break\n",
    "                if possible_answer == '':\n",
    "                    possible_answer = substr\n",
    "\n",
    "            elif 'reason' in sent:\n",
    "                score += 2\n",
    "                index = sent.index('reason')\n",
    "                substr = sent[index+7:]\n",
    "                span = nlp(substr)\n",
    "                for chunk in span.noun_chunks:\n",
    "                    possible_answer = chunk.text\n",
    "                    break\n",
    "                if possible_answer == '':\n",
    "                    index = substr.find('is')\n",
    "                    if index != -1:\n",
    "                        possible_answer = substr[index+3]\n",
    "                    else:\n",
    "                        index = substr.find('was')\n",
    "                        if index != -1:\n",
    "                            possible_answer = substr[index+4]\n",
    "                        else:\n",
    "                            possible_answer = sent[sent.index('reason'):]\n",
    "\n",
    "            elif 'for' in sent:\n",
    "                score += 1\n",
    "                index = sent.index('for')\n",
    "                substr = sent[index + 4:]\n",
    "                span = nlp(substr)\n",
    "                for chunk in span.noun_chunks:\n",
    "                    possible_answer = chunk.text\n",
    "                    break\n",
    "                if possible_answer == '':\n",
    "                    possible_answer = substr\n",
    "\n",
    "            elif 'since' in sent:\n",
    "                score += 1\n",
    "                index = sent.index('since')\n",
    "                substr = sent[index + 6:]\n",
    "                possible_answer = substr\n",
    "\n",
    "            if possible_answer != '' and score > max_score:\n",
    "                answer = possible_answer\n",
    "                max_score = score\n",
    "\n",
    "    # if answer not found, find noun phrases\n",
    "    if answer == '':\n",
    "        for np in doc.noun_chunks:\n",
    "            score = 0\n",
    "\n",
    "            np_dep = np.root.dep_\n",
    "            np_head = lemmatize(strip_punctuation(np.root.head.text))\n",
    "            np_head_dep = np.root.head.dep_\n",
    "\n",
    "            if np_dep == dep:\n",
    "                score += 1\n",
    "            if np_head == head:\n",
    "                score += 1\n",
    "            if np_head_dep == head_dep:\n",
    "                score += 1\n",
    "\n",
    "            if np.text not in question:\n",
    "                score += 1\n",
    "\n",
    "            if strip_punctuation(np.text).strip().lower() not in stop:\n",
    "                score += 1\n",
    "                \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                answer = np.text\n",
    "                \n",
    "    a = nltk.word_tokenize(answer)\n",
    "    if len(a) > 0 and a[0].lower() in stop:\n",
    "        del a[0]\n",
    "        answer = ''\n",
    "        for i in range(len(a)):\n",
    "            if i+1 < len(a) and a[i+1] == \"'s\":\n",
    "                answer += a[i]\n",
    "            else:\n",
    "                answer += a[i] + ' '\n",
    "        \n",
    "#     i = 0\n",
    "#     ques_tokens = [lemmatize(token.lower()) for token in nltk.word_tokenize(question)]\n",
    "#     while i < len(a):\n",
    "#         if lemmatize(a[i].lower()) in ques_tokens and a[i].lower() not in stop and a[i] not in punc and len(a) > 1:\n",
    "#             if i+1 < len(a) and a[i+1] == \"'s\":\n",
    "#                 del a[i]\n",
    "#                 del a[i]\n",
    "#             else:\n",
    "#                 del a[i]\n",
    "#         else:\n",
    "#             if i+1 < len(a) and a[i+1] == \"'s\":\n",
    "#                 answer += a[i]\n",
    "#             else:\n",
    "#                 answer += a[i] + ' '\n",
    "#             i += 1\n",
    "        \n",
    "    answer = answer.strip().lower()\n",
    "    answer_result = []\n",
    "    answer_result.append(case_count)\n",
    "    answer_result.append(answer)\n",
    "    writer.writerow(answer_result)\n",
    "    print(case_count,' ',answer)\n",
    "    case_count += 1\n",
    "    \n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  准确率测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
